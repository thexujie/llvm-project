; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
; RUN: llc -mtriple=amdgcn -mcpu=gfx90a -verify-machineinstrs -misched-cluster=0  < %s | FileCheck -check-prefix=GCN %s

define amdgpu_kernel void @rule2(ptr addrspace(7) noalias %in, ptr addrspace(3) noalias %out, i32 %idx0, i32 %idx1, i32 %idx2, i32 %idx3, i32 %val) #0 {
; GCN-LABEL: rule2:
; GCN:       ; %bb.0: ; %entry
; GCN-NEXT:    s_load_dword s12, s[0:1], 0x34
; GCN-NEXT:    s_load_dwordx4 s[8:11], s[0:1], 0x48
; GCN-NEXT:    s_load_dwordx4 s[4:7], s[0:1], 0x24
; GCN-NEXT:    s_waitcnt lgkmcnt(0)
; GCN-NEXT:    v_mad_u32_u24 v2, v0, 24, s12
; GCN-NEXT:    v_mad_u64_u32 v[10:11], s[2:3], s8, 24, v[2:3]
; GCN-NEXT:    v_mad_u64_u32 v[12:13], s[2:3], s9, 24, v[2:3]
; GCN-NEXT:    v_mad_u64_u32 v[14:15], s[2:3], s10, 24, v[2:3]
; GCN-NEXT:    v_mad_u64_u32 v[16:17], s[2:3], s11, 24, v[2:3]
; GCN-NEXT:    buffer_load_dwordx2 v[6:7], v10, s[4:7], 0 offen
; GCN-NEXT:    buffer_load_dwordx2 v[8:9], v12, s[4:7], 0 offen
; GCN-NEXT:    buffer_load_dwordx2 v[2:3], v14, s[4:7], 0 offen
; GCN-NEXT:    buffer_load_dwordx2 v[4:5], v16, s[4:7], 0 offen
; GCN-NEXT:    s_load_dword s13, s[0:1], 0x44
; GCN-NEXT:    s_load_dword s2, s[0:1], 0x58
; GCN-NEXT:    s_lshl_b32 s0, s8, 5
; GCN-NEXT:    s_lshl_b32 s1, s8, 2
; GCN-NEXT:    s_lshl_b32 s3, s9, 5
; GCN-NEXT:    s_lshl_b32 s8, s9, 2
; GCN-NEXT:    s_lshl_b32 s9, s10, 5
; GCN-NEXT:    s_lshl_b32 s10, s10, 2
; GCN-NEXT:    s_lshl_b32 s14, s11, 5
; GCN-NEXT:    s_lshl_b32 s11, s11, 2
; GCN-NEXT:    s_add_i32 s0, s0, s12
; GCN-NEXT:    s_waitcnt lgkmcnt(0)
; GCN-NEXT:    s_add_i32 s1, s1, s13
; GCN-NEXT:    s_add_i32 s3, s3, s12
; GCN-NEXT:    s_add_i32 s8, s8, s13
; GCN-NEXT:    s_add_i32 s9, s9, s12
; GCN-NEXT:    s_add_i32 s10, s10, s13
; GCN-NEXT:    s_add_i32 s12, s14, s12
; GCN-NEXT:    s_add_i32 s11, s11, s13
; GCN-NEXT:    v_lshlrev_b32_e32 v0, 5, v0
; GCN-NEXT:    s_add_i32 s0, s0, 32
; GCN-NEXT:    s_add_i32 s1, s1, 4
; GCN-NEXT:    s_add_i32 s3, s3, 32
; GCN-NEXT:    s_add_i32 s8, s8, 4
; GCN-NEXT:    s_add_i32 s9, s9, 32
; GCN-NEXT:    s_add_i32 s10, s10, 4
; GCN-NEXT:    s_add_i32 s12, s12, 32
; GCN-NEXT:    s_add_i32 s11, s11, 4
; GCN-NEXT:    s_mov_b32 s13, 0x5040100
; GCN-NEXT:    s_mov_b32 s14, 0x7060302
; GCN-NEXT:  .LBB0_1: ; %bb.1
; GCN-NEXT:    ; =>This Inner Loop Header: Depth=1
; GCN-NEXT:    s_waitcnt vmcnt(0)
; GCN-NEXT:    v_perm_b32 v11, v9, v8, s13
; GCN-NEXT:    v_perm_b32 v10, v7, v6, s13
; GCN-NEXT:    v_perm_b32 v9, v9, v8, s14
; GCN-NEXT:    v_perm_b32 v8, v7, v6, s14
; GCN-NEXT:    v_add_u32_e32 v1, s1, v0
; GCN-NEXT:    v_add_u32_e32 v6, s8, v0
; GCN-NEXT:    v_add_u32_e32 v7, s10, v0
; GCN-NEXT:    v_add_u32_e32 v12, s11, v0
; GCN-NEXT:    ds_write_b64 v1, v[10:11]
; GCN-NEXT:    ds_write_b64 v6, v[8:9]
; GCN-NEXT:    s_waitcnt vmcnt(1)
; GCN-NEXT:    ds_write_b64 v7, v[2:3]
; GCN-NEXT:    s_waitcnt vmcnt(0)
; GCN-NEXT:    ds_write_b64 v12, v[4:5]
; GCN-NEXT:    v_add_u32_e32 v1, s0, v0
; GCN-NEXT:    v_add_u32_e32 v10, s3, v0
; GCN-NEXT:    v_add_u32_e32 v11, s9, v0
; GCN-NEXT:    v_add_u32_e32 v12, s12, v0
; GCN-NEXT:    buffer_load_dwordx2 v[2:3], v11, s[4:7], 0 offen
; GCN-NEXT:    buffer_load_dwordx2 v[4:5], v12, s[4:7], 0 offen
; GCN-NEXT:    buffer_load_dwordx2 v[6:7], v1, s[4:7], 0 offen
; GCN-NEXT:    buffer_load_dwordx2 v[8:9], v10, s[4:7], 0 offen
; GCN-NEXT:    s_add_i32 s2, s2, 1
; GCN-NEXT:    s_add_i32 s0, s0, 32
; GCN-NEXT:    s_add_i32 s1, s1, 4
; GCN-NEXT:    s_add_i32 s3, s3, 32
; GCN-NEXT:    s_add_i32 s8, s8, 4
; GCN-NEXT:    s_add_i32 s9, s9, 32
; GCN-NEXT:    s_add_i32 s10, s10, 4
; GCN-NEXT:    s_add_i32 s12, s12, 32
; GCN-NEXT:    s_add_i32 s11, s11, 4
; GCN-NEXT:    s_cmp_lt_u32 s2, 15
; GCN-NEXT:    ; kill: killed $vgpr12
; GCN-NEXT:    ; kill: killed $vgpr10
; GCN-NEXT:    ; kill: killed $vgpr11
; GCN-NEXT:    ; kill: killed $vgpr1
; GCN-NEXT:    ; sched_group_barrier mask(0x00000020) size(1) SyncID(1) RuleMask(0x00000000000002)
; GCN-NEXT:    ; sched_group_barrier mask(0x00000020) size(1) SyncID(1) RuleMask(0x00000000000002)
; GCN-NEXT:    ; sched_group_barrier mask(0x00000020) size(1) SyncID(1)
; GCN-NEXT:    ; sched_group_barrier mask(0x00000020) size(1) SyncID(1)
; GCN-NEXT:    s_cbranch_scc1 .LBB0_1
; GCN-NEXT:  ; %bb.2: ; %bb.2
; GCN-NEXT:    s_endpgm
entry:
  %tid = call i32 @llvm.amdgcn.workitem.id.x() #2
  %gepPtr = getelementptr ptr addrspace(7), ptr addrspace(7) %in, i32 %tid
  %outPtr = getelementptr ptr addrspace(7), ptr addrspace(3) %out, i32 %tid
  %gep0 =  getelementptr ptr addrspace(7), ptr addrspace(7) %gepPtr, i32 %idx0
  %gep1 =  getelementptr ptr addrspace(7), ptr addrspace(7) %gepPtr, i32 %idx1
  %gep2 =  getelementptr ptr addrspace(7), ptr addrspace(7) %gepPtr, i32 %idx2
  %gep3 =  getelementptr ptr addrspace(7), ptr addrspace(7) %gepPtr, i32 %idx3
  %load0 = load <4 x i16>, ptr addrspace(7) %gep0
  %load1 = load <4 x i16>, ptr addrspace(7) %gep1
  %load2 = load <4 x i16>, ptr addrspace(7) %gep2
  %load3 = load <4 x i16>, ptr addrspace(7) %gep3
  br label %bb.1

bb.1:
  %p0 = phi <4 x i16> [ %load0, %entry ], [ %load4, %bb.1 ]
  %p1 = phi <4 x i16> [ %load1, %entry ], [ %load5, %bb.1 ]
  %p2 = phi <4 x i16> [ %load2, %entry ], [ %load6, %bb.1 ]
  %p3 = phi <4 x i16> [ %load3, %entry ], [ %load7, %bb.1 ]
  %val1 = phi i32 [%val, %entry], [%val2, %bb.1]
  %idx8 = phi i32 [%idx0, %entry], [%idx4, %bb.1]
  %idx9 = phi i32 [%idx1, %entry], [%idx5, %bb.1]
  %idx10 = phi i32 [%idx2, %entry], [%idx6, %bb.1]
  %idx11 = phi i32 [%idx3, %entry], [%idx7, %bb.1]
  %shuffle1 = shufflevector <4 x i16> %p0, <4 x i16> %p1, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %shuffle2 = shufflevector <4 x i16> %p0, <4 x i16> %p1, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %idx4 = add i32 %idx8, 1
  %idx5 = add i32 %idx9, 1
  %idx6 = add i32 %idx10, 1
  %idx7 = add i32 %idx11, 1
  %out0 =  getelementptr ptr addrspace(3), ptr addrspace(3) %outPtr, i32 %idx4
  %out1 =  getelementptr ptr addrspace(3), ptr addrspace(3) %outPtr, i32 %idx5
  %out2 =  getelementptr ptr addrspace(3), ptr addrspace(3) %outPtr, i32 %idx6
  %out3 =  getelementptr ptr addrspace(3), ptr addrspace(3) %outPtr, i32 %idx7
  store <4 x i16> %shuffle1, ptr addrspace(3) %out0
  store <4 x i16> %shuffle2, ptr addrspace(3) %out1
  store <4 x i16> %p2, ptr addrspace(3) %out2
  store <4 x i16> %p3, ptr addrspace(3) %out3
  %gep4 =  getelementptr ptr addrspace(7), ptr addrspace(7) %gepPtr, i32 %idx4
  %gep5 =  getelementptr ptr addrspace(7), ptr addrspace(7) %gepPtr, i32 %idx5
  %gep6 =  getelementptr ptr addrspace(7), ptr addrspace(7) %gepPtr, i32 %idx6
  %gep7 =  getelementptr ptr addrspace(7), ptr addrspace(7) %gepPtr, i32 %idx7
  %load4 = load <4 x i16>, ptr addrspace(7) %gep4
  %load5 = load <4 x i16>, ptr addrspace(7) %gep5
  %load6 = load <4 x i16>, ptr addrspace(7) %gep6
  %load7 = load <4 x i16>, ptr addrspace(7) %gep7
  call void @llvm.amdgcn.sched.group.barrier.rule(i32 32, i32 1, i32 1, i64 2)
  call void @llvm.amdgcn.sched.group.barrier.rule(i32 32, i32 1, i32 1, i64 2)
  call void @llvm.amdgcn.sched.group.barrier(i32 32, i32 1, i32 1)
  call void @llvm.amdgcn.sched.group.barrier(i32 32, i32 1, i32 1)
  %val2 = add i32 %val1, 1
  %cmp = icmp ult i32 %val2, 15
  br i1 %cmp, label %bb.1, label %bb.2

bb.2:
  ret void
}

declare void @llvm.amdgcn.sched.group.barrier(i32, i32, i32) #1
declare void @llvm.amdgcn.sched.group.barrier.rule(i32, i32, i32, i64) #1


