; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 3
; RUN: llc %s -mtriple=thumbv8m.main     -o - | FileCheck %s --check-prefix V8M
; RUN: llc %s -mtriple=thumbebv8m.main   -o - | FileCheck %s --check-prefix V8M
; RUN: llc %s -mtriple=thumbv8.1m.main   -o - | FileCheck %s --check-prefix V81M
; RUN: llc %s -mtriple=thumbebv8.1m.main -o - | FileCheck %s --check-prefix V81M

@arr = hidden local_unnamed_addr global [256 x i32] zeroinitializer, align 4

define i32 @access_i16(i16 signext %idx) "cmse_nonsecure_entry" {
; V8M-LABEL: access_i16:
; V8M:       @ %bb.0: @ %entry
; V8M-NEXT:    movw r1, :lower16:arr
; V8M-NEXT:    sxth r0, r0
; V8M-NEXT:    movt r1, :upper16:arr
; V8M-NEXT:    mov r2, lr
; V8M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V8M-NEXT:    mov r1, lr
; V8M-NEXT:    mov r3, lr
; V8M-NEXT:    msr apsr_nzcvq, lr
; V8M-NEXT:    mov r12, lr
; V8M-NEXT:    bxns lr
;
; V81M-LABEL: access_i16:
; V81M:       @ %bb.0: @ %entry
; V81M-NEXT:    vstr fpcxtns, [sp, #-4]!
; V81M-NEXT:    movw r1, :lower16:arr
; V81M-NEXT:    sxth r0, r0
; V81M-NEXT:    movt r1, :upper16:arr
; V81M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V81M-NEXT:    vscclrm {s0, s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12, s13, s14, s15, vpr}
; V81M-NEXT:    vldr fpcxtns, [sp], #4
; V81M-NEXT:    clrm {r1, r2, r3, r12, apsr}
; V81M-NEXT:    bxns lr
entry:
  %idxprom = sext i16 %idx to i32
  %arrayidx = getelementptr inbounds [256 x i32], ptr @arr, i32 0, i32 %idxprom
  %0 = load i32, ptr %arrayidx, align 4
  ret i32 %0
}

define i32 @access_u16(i16 zeroext %idx) "cmse_nonsecure_entry" {
; V8M-LABEL: access_u16:
; V8M:       @ %bb.0: @ %entry
; V8M-NEXT:    movw r1, :lower16:arr
; V8M-NEXT:    uxth r0, r0
; V8M-NEXT:    movt r1, :upper16:arr
; V8M-NEXT:    mov r2, lr
; V8M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V8M-NEXT:    mov r1, lr
; V8M-NEXT:    mov r3, lr
; V8M-NEXT:    msr apsr_nzcvq, lr
; V8M-NEXT:    mov r12, lr
; V8M-NEXT:    bxns lr
;
; V81M-LABEL: access_u16:
; V81M:       @ %bb.0: @ %entry
; V81M-NEXT:    vstr fpcxtns, [sp, #-4]!
; V81M-NEXT:    movw r1, :lower16:arr
; V81M-NEXT:    uxth r0, r0
; V81M-NEXT:    movt r1, :upper16:arr
; V81M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V81M-NEXT:    vscclrm {s0, s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12, s13, s14, s15, vpr}
; V81M-NEXT:    vldr fpcxtns, [sp], #4
; V81M-NEXT:    clrm {r1, r2, r3, r12, apsr}
; V81M-NEXT:    bxns lr
entry:
  %idxprom = zext i16 %idx to i32
  %arrayidx = getelementptr inbounds [256 x i32], ptr @arr, i32 0, i32 %idxprom
  %0 = load i32, ptr %arrayidx, align 4
  ret i32 %0
}

define i32 @access_i8(i8 signext %idx) "cmse_nonsecure_entry" {
; V8M-LABEL: access_i8:
; V8M:       @ %bb.0: @ %entry
; V8M-NEXT:    movw r1, :lower16:arr
; V8M-NEXT:    sxtb r0, r0
; V8M-NEXT:    movt r1, :upper16:arr
; V8M-NEXT:    mov r2, lr
; V8M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V8M-NEXT:    mov r1, lr
; V8M-NEXT:    mov r3, lr
; V8M-NEXT:    msr apsr_nzcvq, lr
; V8M-NEXT:    mov r12, lr
; V8M-NEXT:    bxns lr
;
; V81M-LABEL: access_i8:
; V81M:       @ %bb.0: @ %entry
; V81M-NEXT:    vstr fpcxtns, [sp, #-4]!
; V81M-NEXT:    movw r1, :lower16:arr
; V81M-NEXT:    sxtb r0, r0
; V81M-NEXT:    movt r1, :upper16:arr
; V81M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V81M-NEXT:    vscclrm {s0, s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12, s13, s14, s15, vpr}
; V81M-NEXT:    vldr fpcxtns, [sp], #4
; V81M-NEXT:    clrm {r1, r2, r3, r12, apsr}
; V81M-NEXT:    bxns lr
entry:
  %idxprom = sext i8 %idx to i32
  %arrayidx = getelementptr inbounds [256 x i32], ptr @arr, i32 0, i32 %idxprom
  %0 = load i32, ptr %arrayidx, align 4
  ret i32 %0
}

define i32 @access_u8(i8 zeroext %idx) "cmse_nonsecure_entry" {
; V8M-LABEL: access_u8:
; V8M:       @ %bb.0: @ %entry
; V8M-NEXT:    movw r1, :lower16:arr
; V8M-NEXT:    uxtb r0, r0
; V8M-NEXT:    movt r1, :upper16:arr
; V8M-NEXT:    mov r2, lr
; V8M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V8M-NEXT:    mov r1, lr
; V8M-NEXT:    mov r3, lr
; V8M-NEXT:    msr apsr_nzcvq, lr
; V8M-NEXT:    mov r12, lr
; V8M-NEXT:    bxns lr
;
; V81M-LABEL: access_u8:
; V81M:       @ %bb.0: @ %entry
; V81M-NEXT:    vstr fpcxtns, [sp, #-4]!
; V81M-NEXT:    movw r1, :lower16:arr
; V81M-NEXT:    uxtb r0, r0
; V81M-NEXT:    movt r1, :upper16:arr
; V81M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V81M-NEXT:    vscclrm {s0, s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12, s13, s14, s15, vpr}
; V81M-NEXT:    vldr fpcxtns, [sp], #4
; V81M-NEXT:    clrm {r1, r2, r3, r12, apsr}
; V81M-NEXT:    bxns lr
entry:
  %idxprom = zext i8 %idx to i32
  %arrayidx = getelementptr inbounds [256 x i32], ptr @arr, i32 0, i32 %idxprom
  %0 = load i32, ptr %arrayidx, align 4
  ret i32 %0
}

define i32 @access_i1(i1 signext %idx) "cmse_nonsecure_entry" {
; V8M-LABEL: access_i1:
; V8M:       @ %bb.0: @ %entry
; V8M-NEXT:    and r0, r0, #1
; V8M-NEXT:    movw r1, :lower16:arr
; V8M-NEXT:    rsbs r0, r0, #0
; V8M-NEXT:    movt r1, :upper16:arr
; V8M-NEXT:    and r0, r0, #1
; V8M-NEXT:    mov r2, lr
; V8M-NEXT:    mov r3, lr
; V8M-NEXT:    mov r12, lr
; V8M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V8M-NEXT:    mov r1, lr
; V8M-NEXT:    msr apsr_nzcvq, lr
; V8M-NEXT:    bxns lr
;
; V81M-LABEL: access_i1:
; V81M:       @ %bb.0: @ %entry
; V81M-NEXT:    vstr fpcxtns, [sp, #-4]!
; V81M-NEXT:    and r0, r0, #1
; V81M-NEXT:    movw r1, :lower16:arr
; V81M-NEXT:    rsbs r0, r0, #0
; V81M-NEXT:    movt r1, :upper16:arr
; V81M-NEXT:    and r0, r0, #1
; V81M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V81M-NEXT:    vscclrm {s0, s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12, s13, s14, s15, vpr}
; V81M-NEXT:    vldr fpcxtns, [sp], #4
; V81M-NEXT:    clrm {r1, r2, r3, r12, apsr}
; V81M-NEXT:    bxns lr
entry:
  %idxprom = zext i1 %idx to i32
  %arrayidx = getelementptr inbounds [256 x i32], ptr @arr, i32 0, i32 %idxprom
  %0 = load i32, ptr %arrayidx, align 4
  ret i32 %0
}

define i32 @access_i5(i5 signext %idx) "cmse_nonsecure_entry" {
; V8M-LABEL: access_i5:
; V8M:       @ %bb.0: @ %entry
; V8M-NEXT:    movw r1, :lower16:arr
; V8M-NEXT:    sbfx r0, r0, #0, #5
; V8M-NEXT:    movt r1, :upper16:arr
; V8M-NEXT:    mov r2, lr
; V8M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V8M-NEXT:    mov r1, lr
; V8M-NEXT:    mov r3, lr
; V8M-NEXT:    msr apsr_nzcvq, lr
; V8M-NEXT:    mov r12, lr
; V8M-NEXT:    bxns lr
;
; V81M-LABEL: access_i5:
; V81M:       @ %bb.0: @ %entry
; V81M-NEXT:    vstr fpcxtns, [sp, #-4]!
; V81M-NEXT:    movw r1, :lower16:arr
; V81M-NEXT:    sbfx r0, r0, #0, #5
; V81M-NEXT:    movt r1, :upper16:arr
; V81M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V81M-NEXT:    vscclrm {s0, s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12, s13, s14, s15, vpr}
; V81M-NEXT:    vldr fpcxtns, [sp], #4
; V81M-NEXT:    clrm {r1, r2, r3, r12, apsr}
; V81M-NEXT:    bxns lr
entry:
  %idxprom = sext i5 %idx to i32
  %arrayidx = getelementptr inbounds [256 x i32], ptr @arr, i32 0, i32 %idxprom
  %0 = load i32, ptr %arrayidx, align 4
  ret i32 %0
}

define i32 @access_u5(i5 zeroext %idx) "cmse_nonsecure_entry" {
; V8M-LABEL: access_u5:
; V8M:       @ %bb.0: @ %entry
; V8M-NEXT:    movw r1, :lower16:arr
; V8M-NEXT:    and r0, r0, #31
; V8M-NEXT:    movt r1, :upper16:arr
; V8M-NEXT:    mov r2, lr
; V8M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V8M-NEXT:    mov r1, lr
; V8M-NEXT:    mov r3, lr
; V8M-NEXT:    msr apsr_nzcvq, lr
; V8M-NEXT:    mov r12, lr
; V8M-NEXT:    bxns lr
;
; V81M-LABEL: access_u5:
; V81M:       @ %bb.0: @ %entry
; V81M-NEXT:    vstr fpcxtns, [sp, #-4]!
; V81M-NEXT:    movw r1, :lower16:arr
; V81M-NEXT:    and r0, r0, #31
; V81M-NEXT:    movt r1, :upper16:arr
; V81M-NEXT:    ldr.w r0, [r1, r0, lsl #2]
; V81M-NEXT:    vscclrm {s0, s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12, s13, s14, s15, vpr}
; V81M-NEXT:    vldr fpcxtns, [sp], #4
; V81M-NEXT:    clrm {r1, r2, r3, r12, apsr}
; V81M-NEXT:    bxns lr
entry:
  %idxprom = zext i5 %idx to i32
  %arrayidx = getelementptr inbounds [256 x i32], ptr @arr, i32 0, i32 %idxprom
  %0 = load i32, ptr %arrayidx, align 4
  ret i32 %0
}
