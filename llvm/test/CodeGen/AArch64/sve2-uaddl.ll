; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
; RUN: llc -mtriple=aarch64-unknown-linux -mattr=+sve -o - %s | FileCheck --check-prefix=SVE %s
; RUN: llc -mtriple=aarch64-unknown-linux -mattr=+sve2 -o - %s | FileCheck --check-prefix=SVE2 %s

define <vscale x 16 x i16> @foo_noloadSt_scalable_16x8to16x16(
; SVE-LABEL: foo_noloadSt_scalable_16x8to16x16:
; SVE:       // %bb.0:
; SVE-NEXT:    uunpkhi z2.h, z0.b
; SVE-NEXT:    uunpklo z0.h, z0.b
; SVE-NEXT:    uunpkhi z3.h, z1.b
; SVE-NEXT:    uunpklo z1.h, z1.b
; SVE-NEXT:    add z0.h, z1.h, z0.h
; SVE-NEXT:    add z1.h, z3.h, z2.h
; SVE-NEXT:    ret
;
; SVE2-LABEL: foo_noloadSt_scalable_16x8to16x16:
; SVE2:       // %bb.0:
; SVE2-NEXT:    uaddlt z2.h, z1.b, z0.b
; SVE2-NEXT:    uaddlb z1.h, z1.b, z0.b
; SVE2-NEXT:    zip1 z0.h, z1.h, z2.h
; SVE2-NEXT:    zip2 z1.h, z1.h, z2.h
; SVE2-NEXT:    ret
    <vscale x 16 x i8> %A,
    <vscale x 16 x i8> %B
    ) {
  %1 = zext <vscale x 16 x i8> %A to <vscale x 16 x i16>
  %2 = zext <vscale x 16 x i8> %B to <vscale x 16 x i16>
  %add1 = add nuw nsw <vscale x 16 x i16> %2, %1
  ret <vscale x 16 x i16> %add1
}

define <vscale x 16 x i32> @foo_noloadSt_scalable_16x8to16x32(
; SVE-LABEL: foo_noloadSt_scalable_16x8to16x32:
; SVE:       // %bb.0:
; SVE-NEXT:    uunpklo z4.h, z1.b
; SVE-NEXT:    uunpklo z5.h, z0.b
; SVE-NEXT:    uunpkhi z0.h, z0.b
; SVE-NEXT:    uunpkhi z1.h, z1.b
; SVE-NEXT:    uunpklo z26.s, z2.h
; SVE-NEXT:    uunpkhi z2.s, z2.h
; SVE-NEXT:    uunpklo z6.s, z5.h
; SVE-NEXT:    uunpklo z7.s, z4.h
; SVE-NEXT:    uunpkhi z5.s, z5.h
; SVE-NEXT:    uunpklo z24.s, z0.h
; SVE-NEXT:    uunpkhi z0.s, z0.h
; SVE-NEXT:    uunpkhi z4.s, z4.h
; SVE-NEXT:    uunpklo z25.s, z1.h
; SVE-NEXT:    uunpkhi z1.s, z1.h
; SVE-NEXT:    add z6.s, z7.s, z6.s
; SVE-NEXT:    uunpkhi z7.s, z3.h
; SVE-NEXT:    uunpklo z3.s, z3.h
; SVE-NEXT:    add z27.s, z1.s, z0.s
; SVE-NEXT:    add z24.s, z25.s, z24.s
; SVE-NEXT:    add z1.s, z4.s, z5.s
; SVE-NEXT:    add z0.s, z6.s, z26.s
; SVE-NEXT:    add z1.s, z1.s, z2.s
; SVE-NEXT:    add z2.s, z24.s, z3.s
; SVE-NEXT:    add z3.s, z27.s, z7.s
; SVE-NEXT:    ret
;
; SVE2-LABEL: foo_noloadSt_scalable_16x8to16x32:
; SVE2:       // %bb.0:
; SVE2-NEXT:    uaddlt z4.h, z1.b, z0.b
; SVE2-NEXT:    uaddlb z0.h, z1.b, z0.b
; SVE2-NEXT:    zip1 z1.h, z0.h, z4.h
; SVE2-NEXT:    zip2 z0.h, z0.h, z4.h
; SVE2-NEXT:    uaddlt z4.s, z1.h, z2.h
; SVE2-NEXT:    uaddlb z1.s, z1.h, z2.h
; SVE2-NEXT:    uaddlt z5.s, z0.h, z3.h
; SVE2-NEXT:    uaddlb z3.s, z0.h, z3.h
; SVE2-NEXT:    zip1 z0.s, z1.s, z4.s
; SVE2-NEXT:    zip2 z1.s, z1.s, z4.s
; SVE2-NEXT:    zip1 z2.s, z3.s, z5.s
; SVE2-NEXT:    zip2 z3.s, z3.s, z5.s
; SVE2-NEXT:    ret
    <vscale x 16 x i8> %A,
    <vscale x 16 x i8> %B,
    <vscale x 16 x i16> %C
    ) {
  %1 = zext <vscale x 16 x i8> %A to <vscale x 16 x i32>
  %2 = zext <vscale x 16 x i8> %B to <vscale x 16 x i32>
  %add1 = add nuw nsw <vscale x 16 x i32> %2, %1

  %3 = zext <vscale x 16 x i16> %C to <vscale x 16 x i32>
  %add2 = add nuw nsw <vscale x 16 x i32> %add1, %3
  ret <vscale x 16 x i32> %add2
}

define <vscale x 16 x i64>@foo_noloadSt_scalable_16x8to16x64(
; SVE-LABEL: foo_noloadSt_scalable_16x8to16x64:
; SVE:       // %bb.0:
; SVE-NEXT:    str x29, [sp, #-16]! // 8-byte Folded Spill
; SVE-NEXT:    addvl sp, sp, #-12
; SVE-NEXT:    str z19, [sp] // 16-byte Folded Spill
; SVE-NEXT:    str z18, [sp, #1, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z17, [sp, #2, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z16, [sp, #3, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z15, [sp, #4, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z14, [sp, #5, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z13, [sp, #6, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z12, [sp, #7, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z11, [sp, #8, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z10, [sp, #9, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z9, [sp, #10, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z8, [sp, #11, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    .cfi_escape 0x0f, 0x0d, 0x8f, 0x00, 0x11, 0x10, 0x22, 0x11, 0xe0, 0x00, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 16 + 96 * VG
; SVE-NEXT:    .cfi_offset w29, -16
; SVE-NEXT:    .cfi_escape 0x10, 0x48, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x78, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d8 @ cfa - 16 - 8 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x49, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x70, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d9 @ cfa - 16 - 16 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x4a, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x68, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d10 @ cfa - 16 - 24 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x4b, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x60, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d11 @ cfa - 16 - 32 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x4c, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x58, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d12 @ cfa - 16 - 40 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x4d, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x50, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d13 @ cfa - 16 - 48 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x4e, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x48, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d14 @ cfa - 16 - 56 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x4f, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x40, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d15 @ cfa - 16 - 64 * VG
; SVE-NEXT:    uunpkhi z25.h, z0.b
; SVE-NEXT:    uunpklo z0.h, z0.b
; SVE-NEXT:    uunpklo z26.h, z1.b
; SVE-NEXT:    uunpkhi z1.h, z1.b
; SVE-NEXT:    uunpklo z24.s, z2.h
; SVE-NEXT:    uunpkhi z2.s, z2.h
; SVE-NEXT:    uunpklo z15.s, z3.h
; SVE-NEXT:    uunpkhi z3.s, z3.h
; SVE-NEXT:    uunpkhi z16.d, z4.s
; SVE-NEXT:    uunpklo z4.d, z4.s
; SVE-NEXT:    uunpklo z17.d, z6.s
; SVE-NEXT:    uunpkhi z18.d, z5.s
; SVE-NEXT:    uunpkhi z28.s, z0.h
; SVE-NEXT:    uunpklo z0.s, z0.h
; SVE-NEXT:    uunpklo z29.s, z26.h
; SVE-NEXT:    uunpkhi z26.s, z26.h
; SVE-NEXT:    uunpkhi z27.s, z25.h
; SVE-NEXT:    uunpklo z25.s, z25.h
; SVE-NEXT:    uunpkhi z30.s, z1.h
; SVE-NEXT:    uunpklo z1.s, z1.h
; SVE-NEXT:    uunpklo z5.d, z5.s
; SVE-NEXT:    uunpkhi z19.d, z7.s
; SVE-NEXT:    uunpklo z7.d, z7.s
; SVE-NEXT:    uunpkhi z6.d, z6.s
; SVE-NEXT:    uunpkhi z9.d, z28.s
; SVE-NEXT:    uunpkhi z10.d, z0.s
; SVE-NEXT:    uunpklo z0.d, z0.s
; SVE-NEXT:    uunpkhi z11.d, z29.s
; SVE-NEXT:    uunpklo z29.d, z29.s
; SVE-NEXT:    uunpklo z28.d, z28.s
; SVE-NEXT:    uunpkhi z12.d, z26.s
; SVE-NEXT:    uunpklo z26.d, z26.s
; SVE-NEXT:    uunpkhi z31.d, z27.s
; SVE-NEXT:    uunpklo z27.d, z27.s
; SVE-NEXT:    uunpkhi z8.d, z25.s
; SVE-NEXT:    uunpklo z25.d, z25.s
; SVE-NEXT:    uunpkhi z13.d, z30.s
; SVE-NEXT:    uunpklo z30.d, z30.s
; SVE-NEXT:    uunpkhi z14.d, z1.s
; SVE-NEXT:    uunpklo z1.d, z1.s
; SVE-NEXT:    add z0.d, z29.d, z0.d
; SVE-NEXT:    add z29.d, z11.d, z10.d
; SVE-NEXT:    add z26.d, z26.d, z28.d
; SVE-NEXT:    add z28.d, z12.d, z9.d
; SVE-NEXT:    uunpklo z9.d, z24.s
; SVE-NEXT:    uunpklo z10.d, z2.s
; SVE-NEXT:    uunpklo z11.d, z15.s
; SVE-NEXT:    uunpkhi z12.d, z15.s
; SVE-NEXT:    uunpklo z15.d, z3.s
; SVE-NEXT:    uunpkhi z3.d, z3.s
; SVE-NEXT:    uunpkhi z24.d, z24.s
; SVE-NEXT:    uunpkhi z2.d, z2.s
; SVE-NEXT:    add z25.d, z1.d, z25.d
; SVE-NEXT:    add z8.d, z14.d, z8.d
; SVE-NEXT:    ldr z14, [sp, #5, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z27.d, z30.d, z27.d
; SVE-NEXT:    add z30.d, z13.d, z31.d
; SVE-NEXT:    ldr z13, [sp, #6, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z1.d, z9.d, z4.d
; SVE-NEXT:    add z5.d, z10.d, z5.d
; SVE-NEXT:    ldr z10, [sp, #9, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z6.d, z12.d, z6.d
; SVE-NEXT:    ldr z12, [sp, #7, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z7.d, z15.d, z7.d
; SVE-NEXT:    ldr z15, [sp, #4, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z9.d, z3.d, z19.d
; SVE-NEXT:    ldr z19, [sp] // 16-byte Folded Reload
; SVE-NEXT:    add z4.d, z24.d, z16.d
; SVE-NEXT:    ldr z16, [sp, #3, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z24.d, z2.d, z18.d
; SVE-NEXT:    ldr z18, [sp, #1, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z31.d, z11.d, z17.d
; SVE-NEXT:    ldr z17, [sp, #2, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z11, [sp, #8, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z2.d, z26.d, z5.d
; SVE-NEXT:    add z5.d, z8.d, z6.d
; SVE-NEXT:    ldr z8, [sp, #11, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z6.d, z27.d, z7.d
; SVE-NEXT:    add z7.d, z30.d, z9.d
; SVE-NEXT:    ldr z9, [sp, #10, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z0.d, z0.d, z1.d
; SVE-NEXT:    add z1.d, z29.d, z4.d
; SVE-NEXT:    add z3.d, z28.d, z24.d
; SVE-NEXT:    add z4.d, z25.d, z31.d
; SVE-NEXT:    addvl sp, sp, #12
; SVE-NEXT:    ldr x29, [sp], #16 // 8-byte Folded Reload
; SVE-NEXT:    ret
;
; SVE2-LABEL: foo_noloadSt_scalable_16x8to16x64:
; SVE2:       // %bb.0:
; SVE2-NEXT:    uaddlt z24.h, z1.b, z0.b
; SVE2-NEXT:    uaddlb z0.h, z1.b, z0.b
; SVE2-NEXT:    zip1 z1.h, z0.h, z24.h
; SVE2-NEXT:    zip2 z0.h, z0.h, z24.h
; SVE2-NEXT:    uaddlt z24.s, z1.h, z2.h
; SVE2-NEXT:    uaddlb z1.s, z1.h, z2.h
; SVE2-NEXT:    uaddlt z2.s, z0.h, z3.h
; SVE2-NEXT:    uaddlb z0.s, z0.h, z3.h
; SVE2-NEXT:    zip2 z3.s, z1.s, z24.s
; SVE2-NEXT:    zip1 z1.s, z1.s, z24.s
; SVE2-NEXT:    zip1 z24.s, z0.s, z2.s
; SVE2-NEXT:    zip2 z0.s, z0.s, z2.s
; SVE2-NEXT:    uaddlt z2.d, z1.s, z4.s
; SVE2-NEXT:    uaddlb z1.d, z1.s, z4.s
; SVE2-NEXT:    uaddlt z4.d, z3.s, z5.s
; SVE2-NEXT:    uaddlb z3.d, z3.s, z5.s
; SVE2-NEXT:    uaddlt z5.d, z24.s, z6.s
; SVE2-NEXT:    uaddlb z6.d, z24.s, z6.s
; SVE2-NEXT:    uaddlt z24.d, z0.s, z7.s
; SVE2-NEXT:    uaddlb z7.d, z0.s, z7.s
; SVE2-NEXT:    zip1 z0.d, z1.d, z2.d
; SVE2-NEXT:    zip2 z1.d, z1.d, z2.d
; SVE2-NEXT:    zip1 z2.d, z3.d, z4.d
; SVE2-NEXT:    zip2 z3.d, z3.d, z4.d
; SVE2-NEXT:    zip1 z4.d, z6.d, z5.d
; SVE2-NEXT:    zip2 z5.d, z6.d, z5.d
; SVE2-NEXT:    zip1 z6.d, z7.d, z24.d
; SVE2-NEXT:    zip2 z7.d, z7.d, z24.d
; SVE2-NEXT:    ret
    <vscale x 16 x i8> %A,
    <vscale x 16 x i8> %B,
    <vscale x 16 x i16> %C,
    <vscale x 16 x i32> %D
    ) {
  %1 = zext <vscale x 16 x i8> %A to <vscale x 16 x i64>
  %2 = zext <vscale x 16 x i8> %B to <vscale x 16 x i64>
  %add1 = add nuw nsw <vscale x 16 x i64> %2, %1

  %3 = zext <vscale x 16 x i16> %C to <vscale x 16 x i64>
  %add2 = add nuw nsw <vscale x 16 x i64> %add1, %3

  %4 = zext <vscale x 16 x i32> %D to <vscale x 16 x i64>
  %add3 = add nuw nsw <vscale x 16 x i64> %add2, %4

  ret <vscale x 16 x i64> %add3
}

define <vscale x 16 x i32> @addlong_tree_noloadSt_scalable_16x8to16x32(
; SVE-LABEL: addlong_tree_noloadSt_scalable_16x8to16x32:
; SVE:       // %bb.0:
; SVE-NEXT:    str x29, [sp, #-16]! // 8-byte Folded Spill
; SVE-NEXT:    addvl sp, sp, #-1
; SVE-NEXT:    str z8, [sp] // 16-byte Folded Spill
; SVE-NEXT:    .cfi_escape 0x0f, 0x0c, 0x8f, 0x00, 0x11, 0x10, 0x22, 0x11, 0x08, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 16 + 8 * VG
; SVE-NEXT:    .cfi_offset w29, -16
; SVE-NEXT:    .cfi_escape 0x10, 0x48, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x78, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d8 @ cfa - 16 - 8 * VG
; SVE-NEXT:    uunpklo z4.h, z0.b
; SVE-NEXT:    uunpkhi z0.h, z0.b
; SVE-NEXT:    uunpklo z5.h, z1.b
; SVE-NEXT:    uunpkhi z1.h, z1.b
; SVE-NEXT:    uunpklo z6.h, z2.b
; SVE-NEXT:    uunpkhi z2.h, z2.b
; SVE-NEXT:    uunpklo z7.h, z3.b
; SVE-NEXT:    uunpkhi z3.h, z3.b
; SVE-NEXT:    uunpklo z25.s, z0.h
; SVE-NEXT:    uunpkhi z0.s, z0.h
; SVE-NEXT:    uunpklo z24.s, z4.h
; SVE-NEXT:    uunpklo z27.s, z1.h
; SVE-NEXT:    uunpkhi z1.s, z1.h
; SVE-NEXT:    uunpklo z29.s, z2.h
; SVE-NEXT:    uunpkhi z2.s, z2.h
; SVE-NEXT:    uunpklo z31.s, z3.h
; SVE-NEXT:    uunpkhi z3.s, z3.h
; SVE-NEXT:    uunpkhi z4.s, z4.h
; SVE-NEXT:    uunpklo z26.s, z5.h
; SVE-NEXT:    uunpkhi z5.s, z5.h
; SVE-NEXT:    uunpklo z28.s, z6.h
; SVE-NEXT:    uunpkhi z6.s, z6.h
; SVE-NEXT:    uunpklo z30.s, z7.h
; SVE-NEXT:    uunpkhi z7.s, z7.h
; SVE-NEXT:    add z8.s, z1.s, z0.s
; SVE-NEXT:    add z25.s, z27.s, z25.s
; SVE-NEXT:    add z3.s, z3.s, z2.s
; SVE-NEXT:    add z2.s, z31.s, z29.s
; SVE-NEXT:    add z1.s, z5.s, z4.s
; SVE-NEXT:    add z0.s, z26.s, z24.s
; SVE-NEXT:    add z4.s, z30.s, z28.s
; SVE-NEXT:    add z5.s, z7.s, z6.s
; SVE-NEXT:    add z3.s, z8.s, z3.s
; SVE-NEXT:    ldr z8, [sp] // 16-byte Folded Reload
; SVE-NEXT:    add z2.s, z25.s, z2.s
; SVE-NEXT:    add z0.s, z0.s, z4.s
; SVE-NEXT:    add z1.s, z1.s, z5.s
; SVE-NEXT:    addvl sp, sp, #1
; SVE-NEXT:    ldr x29, [sp], #16 // 8-byte Folded Reload
; SVE-NEXT:    ret
;
; SVE2-LABEL: addlong_tree_noloadSt_scalable_16x8to16x32:
; SVE2:       // %bb.0:
; SVE2-NEXT:    uaddlb z4.h, z3.b, z2.b
; SVE2-NEXT:    uaddlb z5.h, z1.b, z0.b
; SVE2-NEXT:    uaddlt z0.h, z1.b, z0.b
; SVE2-NEXT:    uaddlt z1.h, z3.b, z2.b
; SVE2-NEXT:    uaddlb z3.s, z5.h, z4.h
; SVE2-NEXT:    uaddlt z4.s, z5.h, z4.h
; SVE2-NEXT:    uaddlb z2.s, z0.h, z1.h
; SVE2-NEXT:    uaddlt z6.s, z0.h, z1.h
; SVE2-NEXT:    zip1 z0.s, z3.s, z2.s
; SVE2-NEXT:    zip1 z1.s, z4.s, z6.s
; SVE2-NEXT:    zip2 z2.s, z3.s, z2.s
; SVE2-NEXT:    zip2 z3.s, z4.s, z6.s
; SVE2-NEXT:    ret
    <vscale x 16 x i8> %A,
    <vscale x 16 x i8> %B,
    <vscale x 16 x i8> %C,
    <vscale x 16 x i8> %D
    ) {
  %1 = zext <vscale x 16 x i8> %A to <vscale x 16 x i32>
  %2 = zext <vscale x 16 x i8> %B to <vscale x 16 x i32>
  %add1 = add nuw nsw <vscale x 16 x i32> %2, %1

  %a1 = zext <vscale x 16 x i8> %C to <vscale x 16 x i32>
  %a2 = zext <vscale x 16 x i8> %D to <vscale x 16 x i32>
  %add2 = add nuw nsw <vscale x 16 x i32> %a2, %a1

  %add3 = add nuw nsw <vscale x 16 x i32> %add1, %add2
  ret <vscale x 16 x i32> %add3
}


define <vscale x 16 x i64> @addlong_tree_noloadSt_scalable_16x8to16x64(
; SVE-LABEL: addlong_tree_noloadSt_scalable_16x8to16x64:
; SVE:       // %bb.0:
; SVE-NEXT:    str x29, [sp, #-16]! // 8-byte Folded Spill
; SVE-NEXT:    addvl sp, sp, #-16
; SVE-NEXT:    str z23, [sp] // 16-byte Folded Spill
; SVE-NEXT:    str z22, [sp, #1, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z21, [sp, #2, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z20, [sp, #3, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z19, [sp, #4, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z18, [sp, #5, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z17, [sp, #6, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z16, [sp, #7, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z15, [sp, #8, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z14, [sp, #9, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z13, [sp, #10, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z12, [sp, #11, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z11, [sp, #12, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z10, [sp, #13, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z9, [sp, #14, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    str z8, [sp, #15, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    addvl sp, sp, #-5
; SVE-NEXT:    .cfi_escape 0x0f, 0x0d, 0x8f, 0x00, 0x11, 0x10, 0x22, 0x11, 0xa8, 0x01, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 16 + 168 * VG
; SVE-NEXT:    .cfi_offset w29, -16
; SVE-NEXT:    .cfi_escape 0x10, 0x48, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x78, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d8 @ cfa - 16 - 8 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x49, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x70, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d9 @ cfa - 16 - 16 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x4a, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x68, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d10 @ cfa - 16 - 24 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x4b, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x60, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d11 @ cfa - 16 - 32 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x4c, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x58, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d12 @ cfa - 16 - 40 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x4d, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x50, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d13 @ cfa - 16 - 48 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x4e, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x48, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d14 @ cfa - 16 - 56 * VG
; SVE-NEXT:    .cfi_escape 0x10, 0x4f, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x40, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d15 @ cfa - 16 - 64 * VG
; SVE-NEXT:    uunpkhi z24.h, z0.b
; SVE-NEXT:    uunpklo z0.h, z0.b
; SVE-NEXT:    uunpklo z26.h, z1.b
; SVE-NEXT:    uunpkhi z1.h, z1.b
; SVE-NEXT:    uunpkhi z25.h, z2.b
; SVE-NEXT:    uunpklo z15.h, z2.b
; SVE-NEXT:    uunpkhi z17.h, z3.b
; SVE-NEXT:    uunpklo z18.h, z3.b
; SVE-NEXT:    uunpkhi z21.h, z4.b
; SVE-NEXT:    uunpkhi z28.s, z0.h
; SVE-NEXT:    uunpklo z0.s, z0.h
; SVE-NEXT:    uunpklo z29.s, z26.h
; SVE-NEXT:    uunpkhi z26.s, z26.h
; SVE-NEXT:    uunpkhi z27.s, z24.h
; SVE-NEXT:    uunpklo z24.s, z24.h
; SVE-NEXT:    uunpkhi z30.s, z1.h
; SVE-NEXT:    uunpklo z31.s, z1.h
; SVE-NEXT:    uunpkhi z10.d, z28.s
; SVE-NEXT:    uunpkhi z11.d, z0.s
; SVE-NEXT:    uunpklo z0.d, z0.s
; SVE-NEXT:    uunpkhi z12.d, z29.s
; SVE-NEXT:    uunpklo z29.d, z29.s
; SVE-NEXT:    uunpkhi z13.d, z26.s
; SVE-NEXT:    uunpkhi z1.d, z27.s
; SVE-NEXT:    uunpklo z8.d, z27.s
; SVE-NEXT:    uunpkhi z9.d, z24.s
; SVE-NEXT:    uunpklo z27.d, z24.s
; SVE-NEXT:    uunpklo z28.d, z28.s
; SVE-NEXT:    uunpklo z14.d, z26.s
; SVE-NEXT:    uunpkhi z24.d, z30.s
; SVE-NEXT:    uunpklo z30.d, z30.s
; SVE-NEXT:    uunpklo z16.d, z31.s
; SVE-NEXT:    uunpkhi z31.d, z31.s
; SVE-NEXT:    add z0.d, z29.d, z0.d
; SVE-NEXT:    add z26.d, z12.d, z11.d
; SVE-NEXT:    add z3.d, z13.d, z10.d
; SVE-NEXT:    uunpkhi z29.s, z25.h
; SVE-NEXT:    uunpklo z11.s, z15.h
; SVE-NEXT:    uunpkhi z12.s, z17.h
; SVE-NEXT:    uunpklo z13.s, z18.h
; SVE-NEXT:    add z2.d, z14.d, z28.d
; SVE-NEXT:    str z0, [sp, #4, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    add z27.d, z16.d, z27.d
; SVE-NEXT:    add z30.d, z30.d, z8.d
; SVE-NEXT:    add z28.d, z31.d, z9.d
; SVE-NEXT:    uunpkhi z31.s, z18.h
; SVE-NEXT:    uunpklo z9.s, z17.h
; SVE-NEXT:    uunpkhi z8.d, z29.s
; SVE-NEXT:    uunpkhi z16.d, z11.s
; SVE-NEXT:    uunpklo z11.d, z11.s
; SVE-NEXT:    uunpkhi z17.d, z12.s
; SVE-NEXT:    uunpklo z18.d, z13.s
; SVE-NEXT:    uunpklo z25.s, z25.h
; SVE-NEXT:    uunpkhi z10.s, z15.h
; SVE-NEXT:    uunpklo z29.d, z29.s
; SVE-NEXT:    uunpklo z12.d, z12.s
; SVE-NEXT:    add z24.d, z24.d, z1.d
; SVE-NEXT:    uunpklo z19.d, z31.s
; SVE-NEXT:    uunpkhi z13.d, z13.s
; SVE-NEXT:    uunpkhi z20.d, z9.s
; SVE-NEXT:    uunpklo z9.d, z9.s
; SVE-NEXT:    uunpkhi z31.d, z31.s
; SVE-NEXT:    add z0.d, z18.d, z11.d
; SVE-NEXT:    add z8.d, z17.d, z8.d
; SVE-NEXT:    uunpkhi z14.d, z25.s
; SVE-NEXT:    uunpklo z15.d, z10.s
; SVE-NEXT:    uunpklo z25.d, z25.s
; SVE-NEXT:    uunpkhi z10.d, z10.s
; SVE-NEXT:    add z29.d, z12.d, z29.d
; SVE-NEXT:    add z11.d, z13.d, z16.d
; SVE-NEXT:    str z0, [sp, #1, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    add z0.d, z24.d, z8.d
; SVE-NEXT:    uunpkhi z8.s, z21.h
; SVE-NEXT:    add z12.d, z20.d, z14.d
; SVE-NEXT:    uunpklo z14.h, z4.b
; SVE-NEXT:    uunpklo z20.h, z7.b
; SVE-NEXT:    add z13.d, z19.d, z15.d
; SVE-NEXT:    uunpklo z15.h, z5.b
; SVE-NEXT:    add z9.d, z9.d, z25.d
; SVE-NEXT:    str z0, [sp, #3, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    add z31.d, z31.d, z10.d
; SVE-NEXT:    add z0.d, z30.d, z29.d
; SVE-NEXT:    add z26.d, z26.d, z11.d
; SVE-NEXT:    uunpkhi z10.h, z5.b
; SVE-NEXT:    uunpkhi z7.h, z7.b
; SVE-NEXT:    add z25.d, z27.d, z9.d
; SVE-NEXT:    uunpklo z9.s, z14.h
; SVE-NEXT:    add z5.d, z2.d, z13.d
; SVE-NEXT:    str z0, [sp, #2, mul vl] // 16-byte Folded Spill
; SVE-NEXT:    add z0.d, z28.d, z12.d
; SVE-NEXT:    add z24.d, z3.d, z31.d
; SVE-NEXT:    uunpkhi z31.s, z14.h
; SVE-NEXT:    uunpklo z11.s, z15.h
; SVE-NEXT:    uunpkhi z12.s, z15.h
; SVE-NEXT:    uunpkhi z14.h, z6.b
; SVE-NEXT:    uunpklo z6.h, z6.b
; SVE-NEXT:    uunpklo z30.s, z21.h
; SVE-NEXT:    str z0, [sp] // 16-byte Folded Spill
; SVE-NEXT:    uunpkhi z15.d, z9.s
; SVE-NEXT:    uunpklo z9.d, z9.s
; SVE-NEXT:    uunpkhi z27.d, z8.s
; SVE-NEXT:    uunpklo z28.d, z8.s
; SVE-NEXT:    uunpkhi z8.s, z10.h
; SVE-NEXT:    uunpkhi z13.d, z31.s
; SVE-NEXT:    uunpklo z31.d, z31.s
; SVE-NEXT:    uunpkhi z17.d, z11.s
; SVE-NEXT:    uunpklo z11.d, z11.s
; SVE-NEXT:    uunpklo z18.d, z12.s
; SVE-NEXT:    uunpkhi z12.d, z12.s
; SVE-NEXT:    uunpkhi z19.s, z14.h
; SVE-NEXT:    uunpklo z10.s, z10.h
; SVE-NEXT:    uunpklo z14.s, z14.h
; SVE-NEXT:    uunpkhi z29.d, z30.s
; SVE-NEXT:    uunpklo z30.d, z30.s
; SVE-NEXT:    uunpkhi z16.d, z8.s
; SVE-NEXT:    uunpklo z8.d, z8.s
; SVE-NEXT:    add z9.d, z11.d, z9.d
; SVE-NEXT:    add z11.d, z17.d, z15.d
; SVE-NEXT:    add z31.d, z18.d, z31.d
; SVE-NEXT:    uunpkhi z15.s, z6.h
; SVE-NEXT:    uunpklo z6.s, z6.h
; SVE-NEXT:    add z12.d, z12.d, z13.d
; SVE-NEXT:    uunpkhi z13.d, z19.s
; SVE-NEXT:    uunpklo z17.s, z20.h
; SVE-NEXT:    uunpklo z18.d, z19.s
; SVE-NEXT:    uunpklo z19.s, z7.h
; SVE-NEXT:    uunpkhi z7.s, z7.h
; SVE-NEXT:    uunpkhi z20.s, z20.h
; SVE-NEXT:    uunpkhi z21.d, z10.s
; SVE-NEXT:    uunpklo z10.d, z10.s
; SVE-NEXT:    uunpkhi z22.d, z14.s
; SVE-NEXT:    uunpkhi z4.d, z6.s
; SVE-NEXT:    uunpklo z6.d, z6.s
; SVE-NEXT:    uunpklo z14.d, z14.s
; SVE-NEXT:    uunpkhi z2.d, z17.s
; SVE-NEXT:    uunpklo z17.d, z17.s
; SVE-NEXT:    uunpkhi z23.d, z15.s
; SVE-NEXT:    uunpkhi z3.d, z7.s
; SVE-NEXT:    uunpklo z15.d, z15.s
; SVE-NEXT:    uunpkhi z1.d, z20.s
; SVE-NEXT:    uunpklo z20.d, z20.s
; SVE-NEXT:    uunpklo z0.d, z19.s
; SVE-NEXT:    uunpklo z7.d, z7.s
; SVE-NEXT:    add z30.d, z10.d, z30.d
; SVE-NEXT:    ldr z10, [sp, #4, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    uunpkhi z19.d, z19.s
; SVE-NEXT:    add z6.d, z17.d, z6.d
; SVE-NEXT:    add z28.d, z8.d, z28.d
; SVE-NEXT:    add z2.d, z2.d, z4.d
; SVE-NEXT:    add z3.d, z3.d, z13.d
; SVE-NEXT:    ldr z13, [sp, #1, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z29.d, z21.d, z29.d
; SVE-NEXT:    add z4.d, z20.d, z15.d
; SVE-NEXT:    add z0.d, z0.d, z14.d
; SVE-NEXT:    add z7.d, z7.d, z18.d
; SVE-NEXT:    add z6.d, z9.d, z6.d
; SVE-NEXT:    add z27.d, z16.d, z27.d
; SVE-NEXT:    add z1.d, z1.d, z23.d
; SVE-NEXT:    add z10.d, z10.d, z13.d
; SVE-NEXT:    add z8.d, z19.d, z22.d
; SVE-NEXT:    add z2.d, z11.d, z2.d
; SVE-NEXT:    add z30.d, z30.d, z0.d
; SVE-NEXT:    add z4.d, z31.d, z4.d
; SVE-NEXT:    add z7.d, z28.d, z7.d
; SVE-NEXT:    add z9.d, z12.d, z1.d
; SVE-NEXT:    add z27.d, z27.d, z3.d
; SVE-NEXT:    add z0.d, z10.d, z6.d
; SVE-NEXT:    ldr z6, [sp, #2, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z28.d, z29.d, z8.d
; SVE-NEXT:    add z1.d, z26.d, z2.d
; SVE-NEXT:    add z2.d, z5.d, z4.d
; SVE-NEXT:    ldr z5, [sp] // 16-byte Folded Reload
; SVE-NEXT:    add z3.d, z24.d, z9.d
; SVE-NEXT:    add z4.d, z25.d, z30.d
; SVE-NEXT:    add z6.d, z6.d, z7.d
; SVE-NEXT:    ldr z7, [sp, #3, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    add z5.d, z5.d, z28.d
; SVE-NEXT:    add z7.d, z7.d, z27.d
; SVE-NEXT:    addvl sp, sp, #5
; SVE-NEXT:    ldr z23, [sp] // 16-byte Folded Reload
; SVE-NEXT:    ldr z22, [sp, #1, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z21, [sp, #2, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z20, [sp, #3, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z19, [sp, #4, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z18, [sp, #5, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z17, [sp, #6, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z16, [sp, #7, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z15, [sp, #8, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z14, [sp, #9, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z13, [sp, #10, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z12, [sp, #11, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z11, [sp, #12, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z10, [sp, #13, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z9, [sp, #14, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    ldr z8, [sp, #15, mul vl] // 16-byte Folded Reload
; SVE-NEXT:    addvl sp, sp, #16
; SVE-NEXT:    ldr x29, [sp], #16 // 8-byte Folded Reload
; SVE-NEXT:    ret
;
; SVE2-LABEL: addlong_tree_noloadSt_scalable_16x8to16x64:
; SVE2:       // %bb.0:
; SVE2-NEXT:    uaddlt z24.h, z3.b, z2.b
; SVE2-NEXT:    uaddlt z25.h, z1.b, z0.b
; SVE2-NEXT:    uaddlb z0.h, z1.b, z0.b
; SVE2-NEXT:    uaddlb z1.h, z3.b, z2.b
; SVE2-NEXT:    uaddlt z2.h, z5.b, z4.b
; SVE2-NEXT:    uaddlb z3.h, z5.b, z4.b
; SVE2-NEXT:    uaddlt z4.h, z7.b, z6.b
; SVE2-NEXT:    uaddlb z5.h, z7.b, z6.b
; SVE2-NEXT:    uaddlb z7.s, z25.h, z24.h
; SVE2-NEXT:    uaddlt z24.s, z25.h, z24.h
; SVE2-NEXT:    uaddlb z6.s, z0.h, z1.h
; SVE2-NEXT:    uaddlt z0.s, z0.h, z1.h
; SVE2-NEXT:    uaddlb z26.s, z3.h, z5.h
; SVE2-NEXT:    uaddlb z27.s, z2.h, z4.h
; SVE2-NEXT:    uaddlt z2.s, z2.h, z4.h
; SVE2-NEXT:    uaddlt z1.s, z3.h, z5.h
; SVE2-NEXT:    uaddlb z4.d, z7.s, z27.s
; SVE2-NEXT:    uaddlb z5.d, z6.s, z26.s
; SVE2-NEXT:    uaddlt z7.d, z7.s, z27.s
; SVE2-NEXT:    uaddlt z6.d, z6.s, z26.s
; SVE2-NEXT:    uaddlb z25.d, z24.s, z2.s
; SVE2-NEXT:    uaddlb z26.d, z0.s, z1.s
; SVE2-NEXT:    uaddlt z24.d, z24.s, z2.s
; SVE2-NEXT:    uaddlt z27.d, z0.s, z1.s
; SVE2-NEXT:    zip1 z0.d, z5.d, z4.d
; SVE2-NEXT:    zip2 z4.d, z5.d, z4.d
; SVE2-NEXT:    zip1 z1.d, z6.d, z7.d
; SVE2-NEXT:    zip1 z2.d, z26.d, z25.d
; SVE2-NEXT:    zip2 z5.d, z6.d, z7.d
; SVE2-NEXT:    zip1 z3.d, z27.d, z24.d
; SVE2-NEXT:    zip2 z6.d, z26.d, z25.d
; SVE2-NEXT:    zip2 z7.d, z27.d, z24.d
; SVE2-NEXT:    ret
    <vscale x 16 x i8> %A,
    <vscale x 16 x i8> %B,
    <vscale x 16 x i8> %C,
    <vscale x 16 x i8> %D,
    <vscale x 16 x i8> %E,
    <vscale x 16 x i8> %F,
    <vscale x 16 x i8> %G,
    <vscale x 16 x i8> %H
    ) {

  %1 = zext <vscale x 16 x i8> %A to <vscale x 16 x i64>
  %2 = zext <vscale x 16 x i8> %B to <vscale x 16 x i64>
  %add1 = add nuw nsw <vscale x 16 x i64> %2, %1

  %a1 = zext <vscale x 16 x i8> %C to <vscale x 16 x i64>
  %a2 = zext <vscale x 16 x i8> %D to <vscale x 16 x i64>
  %add2 = add nuw nsw <vscale x 16 x i64> %a2, %a1

  %add3 = add nuw nsw <vscale x 16 x i64> %add1, %add2


  %a1a = zext <vscale x 16 x i8> %E to <vscale x 16 x i64>
  %a2a = zext <vscale x 16 x i8> %F to <vscale x 16 x i64>
  %add1a = add nuw nsw <vscale x 16 x i64> %a2a, %a1a

  %aa1 = zext <vscale x 16 x i8> %G to <vscale x 16 x i64>
  %aa2 = zext <vscale x 16 x i8> %H to <vscale x 16 x i64>
  %add2a = add nuw nsw <vscale x 16 x i64> %aa2, %aa1

  %add3a = add nuw nsw <vscale x 16 x i64> %add1a, %add2a


  %add4 = add nuw nsw <vscale x 16 x i64> %add3, %add3a
  ret <vscale x 16 x i64> %add4
}
