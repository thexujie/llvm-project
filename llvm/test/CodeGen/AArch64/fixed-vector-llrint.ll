; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=aarch64 -mattr=+sve | FileCheck %s

define <1 x i64> @llrint_v1i64_v1f16(<1 x half> %x) {
; CHECK-LABEL: llrint_v1i64_v1f16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    frintx h0, h0
; CHECK-NEXT:    fcvtzs x8, h0
; CHECK-NEXT:    fmov d0, x8
; CHECK-NEXT:    ret
  %a = call <1 x i64> @llvm.llrint.v1i64.v1f16(<1 x half> %x)
  ret <1 x i64> %a
}
declare <1 x i64> @llvm.llrint.v1i64.v1f16(<1 x half>)

define <2 x i64> @llrint_v1i64_v2f16(<2 x half> %x) {
; CHECK-LABEL: llrint_v1i64_v2f16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    // kill: def $d0 killed $d0 def $q0
; CHECK-NEXT:    mov h1, v0.h[1]
; CHECK-NEXT:    frintx h0, h0
; CHECK-NEXT:    frintx h1, h1
; CHECK-NEXT:    fcvtzs x8, h0
; CHECK-NEXT:    fcvtzs x9, h1
; CHECK-NEXT:    fmov d0, x8
; CHECK-NEXT:    mov v0.d[1], x9
; CHECK-NEXT:    ret
  %a = call <2 x i64> @llvm.llrint.v2i64.v2f16(<2 x half> %x)
  ret <2 x i64> %a
}
declare <2 x i64> @llvm.llrint.v2i64.v2f16(<2 x half>)

define <4 x i64> @llrint_v4i64_v4f16(<4 x half> %x) {
; CHECK-LABEL: llrint_v4i64_v4f16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    // kill: def $d0 killed $d0 def $q0
; CHECK-NEXT:    mov h1, v0.h[2]
; CHECK-NEXT:    mov h2, v0.h[1]
; CHECK-NEXT:    mov h3, v0.h[3]
; CHECK-NEXT:    frintx h0, h0
; CHECK-NEXT:    frintx h1, h1
; CHECK-NEXT:    frintx h2, h2
; CHECK-NEXT:    frintx h3, h3
; CHECK-NEXT:    fcvtzs x8, h0
; CHECK-NEXT:    fcvtzs x9, h1
; CHECK-NEXT:    fcvtzs x10, h2
; CHECK-NEXT:    fcvtzs x11, h3
; CHECK-NEXT:    fmov d0, x8
; CHECK-NEXT:    fmov d1, x9
; CHECK-NEXT:    mov v0.d[1], x10
; CHECK-NEXT:    mov v1.d[1], x11
; CHECK-NEXT:    ret
  %a = call <4 x i64> @llvm.llrint.v4i64.v4f16(<4 x half> %x)
  ret <4 x i64> %a
}
declare <4 x i64> @llvm.llrint.v4i64.v4f16(<4 x half>)

define <8 x i64> @llrint_v8i64_v8f16(<8 x half> %x) {
; CHECK-LABEL: llrint_v8i64_v8f16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ext v1.16b, v0.16b, v0.16b, #8
; CHECK-NEXT:    mov h4, v0.h[2]
; CHECK-NEXT:    mov h3, v0.h[1]
; CHECK-NEXT:    mov h7, v0.h[3]
; CHECK-NEXT:    frintx h0, h0
; CHECK-NEXT:    mov h2, v1.h[2]
; CHECK-NEXT:    mov h5, v1.h[1]
; CHECK-NEXT:    mov h6, v1.h[3]
; CHECK-NEXT:    frintx h1, h1
; CHECK-NEXT:    frintx h4, h4
; CHECK-NEXT:    frintx h3, h3
; CHECK-NEXT:    frintx h7, h7
; CHECK-NEXT:    fcvtzs x9, h0
; CHECK-NEXT:    frintx h2, h2
; CHECK-NEXT:    frintx h5, h5
; CHECK-NEXT:    frintx h6, h6
; CHECK-NEXT:    fcvtzs x8, h1
; CHECK-NEXT:    fcvtzs x12, h4
; CHECK-NEXT:    fcvtzs x11, h3
; CHECK-NEXT:    fcvtzs x15, h7
; CHECK-NEXT:    fmov d0, x9
; CHECK-NEXT:    fcvtzs x10, h2
; CHECK-NEXT:    fcvtzs x13, h5
; CHECK-NEXT:    fcvtzs x14, h6
; CHECK-NEXT:    fmov d2, x8
; CHECK-NEXT:    fmov d1, x12
; CHECK-NEXT:    mov v0.d[1], x11
; CHECK-NEXT:    fmov d3, x10
; CHECK-NEXT:    mov v2.d[1], x13
; CHECK-NEXT:    mov v1.d[1], x15
; CHECK-NEXT:    mov v3.d[1], x14
; CHECK-NEXT:    ret
  %a = call <8 x i64> @llvm.llrint.v8i64.v8f16(<8 x half> %x)
  ret <8 x i64> %a
}
declare <8 x i64> @llvm.llrint.v8i64.v8f16(<8 x half>)

define <16 x i64> @llrint_v16i64_v16f16(<16 x half> %x) {
; CHECK-LABEL: llrint_v16i64_v16f16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ext v2.16b, v0.16b, v0.16b, #8
; CHECK-NEXT:    ext v3.16b, v1.16b, v1.16b, #8
; CHECK-NEXT:    mov h4, v0.h[1]
; CHECK-NEXT:    frintx h5, h0
; CHECK-NEXT:    mov h18, v0.h[2]
; CHECK-NEXT:    mov h0, v0.h[3]
; CHECK-NEXT:    frintx h6, h2
; CHECK-NEXT:    mov h7, v2.h[1]
; CHECK-NEXT:    mov h16, v2.h[2]
; CHECK-NEXT:    mov h17, v3.h[2]
; CHECK-NEXT:    frintx h19, h3
; CHECK-NEXT:    frintx h4, h4
; CHECK-NEXT:    fcvtzs x8, h5
; CHECK-NEXT:    mov h5, v1.h[1]
; CHECK-NEXT:    mov h2, v2.h[3]
; CHECK-NEXT:    frintx h18, h18
; CHECK-NEXT:    frintx h0, h0
; CHECK-NEXT:    fcvtzs x9, h6
; CHECK-NEXT:    frintx h6, h7
; CHECK-NEXT:    frintx h7, h16
; CHECK-NEXT:    mov h16, v1.h[2]
; CHECK-NEXT:    frintx h17, h17
; CHECK-NEXT:    fcvtzs x10, h19
; CHECK-NEXT:    mov h19, v3.h[1]
; CHECK-NEXT:    fcvtzs x11, h4
; CHECK-NEXT:    mov h4, v1.h[3]
; CHECK-NEXT:    mov h3, v3.h[3]
; CHECK-NEXT:    frintx h1, h1
; CHECK-NEXT:    frintx h5, h5
; CHECK-NEXT:    fcvtzs x13, h7
; CHECK-NEXT:    fcvtzs x12, h6
; CHECK-NEXT:    fcvtzs x15, h18
; CHECK-NEXT:    frintx h7, h16
; CHECK-NEXT:    fcvtzs x14, h17
; CHECK-NEXT:    frintx h16, h2
; CHECK-NEXT:    frintx h17, h19
; CHECK-NEXT:    frintx h4, h4
; CHECK-NEXT:    fmov d2, x9
; CHECK-NEXT:    frintx h19, h3
; CHECK-NEXT:    fcvtzs x9, h1
; CHECK-NEXT:    fmov d6, x10
; CHECK-NEXT:    fmov d3, x13
; CHECK-NEXT:    fcvtzs x13, h0
; CHECK-NEXT:    fcvtzs x16, h5
; CHECK-NEXT:    fcvtzs x10, h7
; CHECK-NEXT:    fmov d7, x14
; CHECK-NEXT:    fcvtzs x14, h16
; CHECK-NEXT:    fcvtzs x17, h17
; CHECK-NEXT:    fcvtzs x0, h4
; CHECK-NEXT:    fmov d0, x8
; CHECK-NEXT:    fcvtzs x18, h19
; CHECK-NEXT:    fmov d1, x15
; CHECK-NEXT:    fmov d4, x9
; CHECK-NEXT:    mov v2.d[1], x12
; CHECK-NEXT:    fmov d5, x10
; CHECK-NEXT:    mov v0.d[1], x11
; CHECK-NEXT:    mov v3.d[1], x14
; CHECK-NEXT:    mov v1.d[1], x13
; CHECK-NEXT:    mov v4.d[1], x16
; CHECK-NEXT:    mov v6.d[1], x17
; CHECK-NEXT:    mov v7.d[1], x18
; CHECK-NEXT:    mov v5.d[1], x0
; CHECK-NEXT:    ret
  %a = call <16 x i64> @llvm.llrint.v16i64.v16f16(<16 x half> %x)
  ret <16 x i64> %a
}
declare <16 x i64> @llvm.llrint.v16i64.v16f16(<16 x half>)

define <32 x i64> @llrint_v32i64_v32f16(<32 x half> %x) {
; CHECK-LABEL: llrint_v32i64_v32f16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ext v4.16b, v1.16b, v1.16b, #8
; CHECK-NEXT:    ext v5.16b, v2.16b, v2.16b, #8
; CHECK-NEXT:    ext v6.16b, v3.16b, v3.16b, #8
; CHECK-NEXT:    ext v7.16b, v0.16b, v0.16b, #8
; CHECK-NEXT:    frintx h21, h1
; CHECK-NEXT:    frintx h22, h2
; CHECK-NEXT:    mov h26, v2.h[2]
; CHECK-NEXT:    frintx h19, h0
; CHECK-NEXT:    mov h27, v3.h[2]
; CHECK-NEXT:    mov h20, v2.h[1]
; CHECK-NEXT:    mov h18, v1.h[1]
; CHECK-NEXT:    mov h16, v4.h[2]
; CHECK-NEXT:    mov h17, v5.h[2]
; CHECK-NEXT:    frintx h23, h5
; CHECK-NEXT:    frintx h24, h6
; CHECK-NEXT:    mov h25, v6.h[2]
; CHECK-NEXT:    fcvtzs x9, h21
; CHECK-NEXT:    fcvtzs x11, h22
; CHECK-NEXT:    frintx h22, h7
; CHECK-NEXT:    mov h21, v3.h[3]
; CHECK-NEXT:    fcvtzs x10, h19
; CHECK-NEXT:    frintx h27, h27
; CHECK-NEXT:    frintx h20, h20
; CHECK-NEXT:    frintx h16, h16
; CHECK-NEXT:    frintx h17, h17
; CHECK-NEXT:    fcvtzs x12, h23
; CHECK-NEXT:    fcvtzs x13, h24
; CHECK-NEXT:    frintx h23, h25
; CHECK-NEXT:    frintx h25, h26
; CHECK-NEXT:    mov h26, v3.h[1]
; CHECK-NEXT:    mov h24, v2.h[3]
; CHECK-NEXT:    fmov d19, x9
; CHECK-NEXT:    fcvtzs x9, h22
; CHECK-NEXT:    frintx h22, h3
; CHECK-NEXT:    frintx h21, h21
; CHECK-NEXT:    fcvtzs x14, h16
; CHECK-NEXT:    fcvtzs x15, h17
; CHECK-NEXT:    fmov d2, x12
; CHECK-NEXT:    fmov d16, x13
; CHECK-NEXT:    fcvtzs x12, h23
; CHECK-NEXT:    fcvtzs x13, h25
; CHECK-NEXT:    mov h23, v1.h[2]
; CHECK-NEXT:    frintx h25, h26
; CHECK-NEXT:    frintx h24, h24
; CHECK-NEXT:    mov h1, v1.h[3]
; CHECK-NEXT:    fmov d26, x11
; CHECK-NEXT:    fcvtzs x11, h21
; CHECK-NEXT:    fmov d3, x14
; CHECK-NEXT:    fmov d17, x15
; CHECK-NEXT:    fcvtzs x14, h22
; CHECK-NEXT:    fcvtzs x15, h27
; CHECK-NEXT:    mov h22, v0.h[2]
; CHECK-NEXT:    frintx h18, h18
; CHECK-NEXT:    frintx h21, h23
; CHECK-NEXT:    fmov d23, x13
; CHECK-NEXT:    fcvtzs x13, h25
; CHECK-NEXT:    frintx h1, h1
; CHECK-NEXT:    fmov d25, x14
; CHECK-NEXT:    fcvtzs x14, h24
; CHECK-NEXT:    fmov d24, x15
; CHECK-NEXT:    frintx h22, h22
; CHECK-NEXT:    fcvtzs x15, h18
; CHECK-NEXT:    mov h18, v7.h[1]
; CHECK-NEXT:    mov v25.d[1], x13
; CHECK-NEXT:    fcvtzs x13, h21
; CHECK-NEXT:    mov h21, v7.h[2]
; CHECK-NEXT:    mov v24.d[1], x11
; CHECK-NEXT:    fcvtzs x11, h20
; CHECK-NEXT:    mov h20, v0.h[1]
; CHECK-NEXT:    mov h0, v0.h[3]
; CHECK-NEXT:    mov v23.d[1], x14
; CHECK-NEXT:    fcvtzs x14, h1
; CHECK-NEXT:    mov h1, v6.h[3]
; CHECK-NEXT:    mov h6, v6.h[1]
; CHECK-NEXT:    mov v19.d[1], x15
; CHECK-NEXT:    mov h7, v7.h[3]
; CHECK-NEXT:    stp q25, q24, [x8, #192]
; CHECK-NEXT:    fmov d24, x13
; CHECK-NEXT:    frintx h20, h20
; CHECK-NEXT:    mov v26.d[1], x11
; CHECK-NEXT:    fcvtzs x11, h22
; CHECK-NEXT:    mov h22, v5.h[1]
; CHECK-NEXT:    mov h5, v5.h[3]
; CHECK-NEXT:    frintx h0, h0
; CHECK-NEXT:    frintx h1, h1
; CHECK-NEXT:    mov v24.d[1], x14
; CHECK-NEXT:    mov h25, v4.h[3]
; CHECK-NEXT:    frintx h6, h6
; CHECK-NEXT:    stp q26, q23, [x8, #128]
; CHECK-NEXT:    fmov d23, x12
; CHECK-NEXT:    fcvtzs x12, h20
; CHECK-NEXT:    mov h20, v4.h[1]
; CHECK-NEXT:    frintx h5, h5
; CHECK-NEXT:    fcvtzs x13, h0
; CHECK-NEXT:    stp q19, q24, [x8, #64]
; CHECK-NEXT:    frintx h22, h22
; CHECK-NEXT:    fmov d0, x10
; CHECK-NEXT:    fmov d19, x11
; CHECK-NEXT:    frintx h4, h4
; CHECK-NEXT:    fcvtzs x10, h1
; CHECK-NEXT:    frintx h1, h21
; CHECK-NEXT:    frintx h24, h25
; CHECK-NEXT:    fcvtzs x11, h6
; CHECK-NEXT:    frintx h20, h20
; CHECK-NEXT:    frintx h6, h7
; CHECK-NEXT:    fcvtzs x14, h5
; CHECK-NEXT:    mov v19.d[1], x13
; CHECK-NEXT:    frintx h5, h18
; CHECK-NEXT:    fcvtzs x13, h22
; CHECK-NEXT:    mov v0.d[1], x12
; CHECK-NEXT:    fcvtzs x12, h4
; CHECK-NEXT:    mov v23.d[1], x10
; CHECK-NEXT:    fcvtzs x10, h1
; CHECK-NEXT:    fcvtzs x15, h24
; CHECK-NEXT:    mov v16.d[1], x11
; CHECK-NEXT:    fcvtzs x11, h20
; CHECK-NEXT:    mov v17.d[1], x14
; CHECK-NEXT:    fcvtzs x14, h6
; CHECK-NEXT:    mov v2.d[1], x13
; CHECK-NEXT:    fcvtzs x13, h5
; CHECK-NEXT:    fmov d4, x9
; CHECK-NEXT:    stp q0, q19, [x8]
; CHECK-NEXT:    fmov d0, x12
; CHECK-NEXT:    stp q16, q23, [x8, #224]
; CHECK-NEXT:    fmov d1, x10
; CHECK-NEXT:    mov v3.d[1], x15
; CHECK-NEXT:    stp q2, q17, [x8, #160]
; CHECK-NEXT:    mov v0.d[1], x11
; CHECK-NEXT:    mov v4.d[1], x13
; CHECK-NEXT:    mov v1.d[1], x14
; CHECK-NEXT:    stp q0, q3, [x8, #96]
; CHECK-NEXT:    stp q4, q1, [x8, #32]
; CHECK-NEXT:    ret
  %a = call <32 x i64> @llvm.llrint.v32i64.v32f16(<32 x half> %x)
  ret <32 x i64> %a
}
declare <32 x i64> @llvm.llrint.v32i64.v32f16(<32 x half>)

define <1 x i64> @llrint_v1i64_v1f32(<1 x float> %x) {
; CHECK-LABEL: llrint_v1i64_v1f32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    // kill: def $d0 killed $d0 def $q0
; CHECK-NEXT:    frintx s0, s0
; CHECK-NEXT:    fcvtzs x8, s0
; CHECK-NEXT:    fmov d0, x8
; CHECK-NEXT:    ret
  %a = call <1 x i64> @llvm.llrint.v1i64.v1f32(<1 x float> %x)
  ret <1 x i64> %a
}
declare <1 x i64> @llvm.llrint.v1i64.v1f32(<1 x float>)

define <2 x i64> @llrint_v2i64_v2f32(<2 x float> %x) {
; CHECK-LABEL: llrint_v2i64_v2f32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    str x29, [sp, #-16]! // 8-byte Folded Spill
; CHECK-NEXT:    addvl sp, sp, #-1
; CHECK-NEXT:    .cfi_escape 0x0f, 0x0c, 0x8f, 0x00, 0x11, 0x10, 0x22, 0x11, 0x08, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 16 + 8 * VG
; CHECK-NEXT:    .cfi_offset w29, -16
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    addpl x8, sp, #4
; CHECK-NEXT:    str d0, [x8]
; CHECK-NEXT:    ld1w { z0.d }, p0/z, [sp, #1, mul vl]
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z0.s
; CHECK-NEXT:    // kill: def $q0 killed $q0 killed $z0
; CHECK-NEXT:    addvl sp, sp, #1
; CHECK-NEXT:    ldr x29, [sp], #16 // 8-byte Folded Reload
; CHECK-NEXT:    ret
  %a = call <2 x i64> @llvm.llrint.v2i64.v2f32(<2 x float> %x)
  ret <2 x i64> %a
}
declare <2 x i64> @llvm.llrint.v2i64.v2f32(<2 x float>)

define <4 x i64> @llrint_v4i64_v4f32(<4 x float> %x) {
; CHECK-LABEL: llrint_v4i64_v4f32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    str x29, [sp, #-16]! // 8-byte Folded Spill
; CHECK-NEXT:    addvl sp, sp, #-1
; CHECK-NEXT:    .cfi_escape 0x0f, 0x0c, 0x8f, 0x00, 0x11, 0x10, 0x22, 0x11, 0x08, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 16 + 8 * VG
; CHECK-NEXT:    .cfi_offset w29, -16
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    ext v1.16b, v0.16b, v0.16b, #8
; CHECK-NEXT:    addpl x8, sp, #4
; CHECK-NEXT:    str d0, [sp]
; CHECK-NEXT:    str d1, [x8]
; CHECK-NEXT:    ld1w { z0.d }, p0/z, [sp]
; CHECK-NEXT:    ld1w { z1.d }, p0/z, [sp, #1, mul vl]
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z0.s
; CHECK-NEXT:    fcvtzs z1.d, p0/m, z1.s
; CHECK-NEXT:    // kill: def $q0 killed $q0 killed $z0
; CHECK-NEXT:    // kill: def $q1 killed $q1 killed $z1
; CHECK-NEXT:    addvl sp, sp, #1
; CHECK-NEXT:    ldr x29, [sp], #16 // 8-byte Folded Reload
; CHECK-NEXT:    ret
  %a = call <4 x i64> @llvm.llrint.v4i64.v4f32(<4 x float> %x)
  ret <4 x i64> %a
}
declare <4 x i64> @llvm.llrint.v4i64.v4f32(<4 x float>)

define <8 x i64> @llrint_v8i64_v8f32(<8 x float> %x) {
; CHECK-LABEL: llrint_v8i64_v8f32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    str x29, [sp, #-16]! // 8-byte Folded Spill
; CHECK-NEXT:    addvl sp, sp, #-2
; CHECK-NEXT:    .cfi_escape 0x0f, 0x0c, 0x8f, 0x00, 0x11, 0x10, 0x22, 0x11, 0x10, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 16 + 16 * VG
; CHECK-NEXT:    .cfi_offset w29, -16
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    ext v2.16b, v0.16b, v0.16b, #8
; CHECK-NEXT:    ext v3.16b, v1.16b, v1.16b, #8
; CHECK-NEXT:    addvl x8, sp, #1
; CHECK-NEXT:    str d0, [sp]
; CHECK-NEXT:    str d1, [x8]
; CHECK-NEXT:    addpl x8, sp, #4
; CHECK-NEXT:    str d2, [x8]
; CHECK-NEXT:    addpl x8, sp, #12
; CHECK-NEXT:    str d3, [x8]
; CHECK-NEXT:    ld1w { z1.d }, p0/z, [sp, #2, mul vl]
; CHECK-NEXT:    ld1w { z3.d }, p0/z, [sp, #1, mul vl]
; CHECK-NEXT:    ld1w { z4.d }, p0/z, [sp, #3, mul vl]
; CHECK-NEXT:    ld1w { z0.d }, p0/z, [sp]
; CHECK-NEXT:    movprfx z2, z1
; CHECK-NEXT:    fcvtzs z2.d, p0/m, z1.s
; CHECK-NEXT:    movprfx z1, z3
; CHECK-NEXT:    fcvtzs z1.d, p0/m, z3.s
; CHECK-NEXT:    movprfx z3, z4
; CHECK-NEXT:    fcvtzs z3.d, p0/m, z4.s
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z0.s
; CHECK-NEXT:    // kill: def $q0 killed $q0 killed $z0
; CHECK-NEXT:    // kill: def $q1 killed $q1 killed $z1
; CHECK-NEXT:    // kill: def $q2 killed $q2 killed $z2
; CHECK-NEXT:    // kill: def $q3 killed $q3 killed $z3
; CHECK-NEXT:    addvl sp, sp, #2
; CHECK-NEXT:    ldr x29, [sp], #16 // 8-byte Folded Reload
; CHECK-NEXT:    ret
  %a = call <8 x i64> @llvm.llrint.v8i64.v8f32(<8 x float> %x)
  ret <8 x i64> %a
}
declare <8 x i64> @llvm.llrint.v8i64.v8f32(<8 x float>)

define <16 x i64> @llrint_v16i64_v16f32(<16 x float> %x) {
; CHECK-LABEL: llrint_v16i64_v16f32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    str x29, [sp, #-16]! // 8-byte Folded Spill
; CHECK-NEXT:    addvl sp, sp, #-4
; CHECK-NEXT:    .cfi_escape 0x0f, 0x0c, 0x8f, 0x00, 0x11, 0x10, 0x22, 0x11, 0x20, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 16 + 32 * VG
; CHECK-NEXT:    .cfi_offset w29, -16
; CHECK-NEXT:    str d0, [sp]
; CHECK-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-NEXT:    addvl x8, sp, #1
; CHECK-NEXT:    ext v4.16b, v1.16b, v1.16b, #8
; CHECK-NEXT:    str d1, [x8]
; CHECK-NEXT:    addvl x8, sp, #2
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    str d2, [x8]
; CHECK-NEXT:    ext v1.16b, v2.16b, v2.16b, #8
; CHECK-NEXT:    addvl x8, sp, #3
; CHECK-NEXT:    ext v2.16b, v3.16b, v3.16b, #8
; CHECK-NEXT:    str d3, [x8]
; CHECK-NEXT:    addpl x8, sp, #4
; CHECK-NEXT:    str d0, [x8]
; CHECK-NEXT:    addpl x8, sp, #12
; CHECK-NEXT:    str d4, [x8]
; CHECK-NEXT:    addpl x8, sp, #20
; CHECK-NEXT:    str d1, [x8]
; CHECK-NEXT:    addpl x8, sp, #28
; CHECK-NEXT:    str d2, [x8]
; CHECK-NEXT:    ld1w { z3.d }, p0/z, [sp, #4, mul vl]
; CHECK-NEXT:    ld1w { z5.d }, p0/z, [sp, #3, mul vl]
; CHECK-NEXT:    ld1w { z7.d }, p0/z, [sp, #5, mul vl]
; CHECK-NEXT:    ld1w { z16.d }, p0/z, [sp, #7, mul vl]
; CHECK-NEXT:    ld1w { z0.d }, p0/z, [sp]
; CHECK-NEXT:    ld1w { z1.d }, p0/z, [sp, #1, mul vl]
; CHECK-NEXT:    ld1w { z2.d }, p0/z, [sp, #2, mul vl]
; CHECK-NEXT:    ld1w { z6.d }, p0/z, [sp, #6, mul vl]
; CHECK-NEXT:    movprfx z4, z3
; CHECK-NEXT:    fcvtzs z4.d, p0/m, z3.s
; CHECK-NEXT:    movprfx z3, z5
; CHECK-NEXT:    fcvtzs z3.d, p0/m, z5.s
; CHECK-NEXT:    movprfx z5, z7
; CHECK-NEXT:    fcvtzs z5.d, p0/m, z7.s
; CHECK-NEXT:    movprfx z7, z16
; CHECK-NEXT:    fcvtzs z7.d, p0/m, z16.s
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z0.s
; CHECK-NEXT:    fcvtzs z1.d, p0/m, z1.s
; CHECK-NEXT:    fcvtzs z2.d, p0/m, z2.s
; CHECK-NEXT:    fcvtzs z6.d, p0/m, z6.s
; CHECK-NEXT:    // kill: def $q0 killed $q0 killed $z0
; CHECK-NEXT:    // kill: def $q1 killed $q1 killed $z1
; CHECK-NEXT:    // kill: def $q2 killed $q2 killed $z2
; CHECK-NEXT:    // kill: def $q6 killed $q6 killed $z6
; CHECK-NEXT:    // kill: def $q3 killed $q3 killed $z3
; CHECK-NEXT:    // kill: def $q4 killed $q4 killed $z4
; CHECK-NEXT:    // kill: def $q5 killed $q5 killed $z5
; CHECK-NEXT:    // kill: def $q7 killed $q7 killed $z7
; CHECK-NEXT:    addvl sp, sp, #4
; CHECK-NEXT:    ldr x29, [sp], #16 // 8-byte Folded Reload
; CHECK-NEXT:    ret
  %a = call <16 x i64> @llvm.llrint.v16i64.v16f32(<16 x float> %x)
  ret <16 x i64> %a
}
declare <16 x i64> @llvm.llrint.v16i64.v16f32(<16 x float>)

define <32 x i64> @llrint_v32i64_v32f32(<32 x float> %x) {
; CHECK-LABEL: llrint_v32i64_v32f32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    str x29, [sp, #-16]! // 8-byte Folded Spill
; CHECK-NEXT:    addvl sp, sp, #-8
; CHECK-NEXT:    .cfi_escape 0x0f, 0x0d, 0x8f, 0x00, 0x11, 0x10, 0x22, 0x11, 0xc0, 0x00, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 16 + 64 * VG
; CHECK-NEXT:    .cfi_offset w29, -16
; CHECK-NEXT:    ext v16.16b, v0.16b, v0.16b, #8
; CHECK-NEXT:    ext v17.16b, v1.16b, v1.16b, #8
; CHECK-NEXT:    addpl x9, sp, #4
; CHECK-NEXT:    ext v18.16b, v2.16b, v2.16b, #8
; CHECK-NEXT:    ext v19.16b, v3.16b, v3.16b, #8
; CHECK-NEXT:    ext v20.16b, v4.16b, v4.16b, #8
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    str d16, [x9]
; CHECK-NEXT:    addpl x9, sp, #12
; CHECK-NEXT:    ext v16.16b, v5.16b, v5.16b, #8
; CHECK-NEXT:    str d17, [x9]
; CHECK-NEXT:    addpl x9, sp, #20
; CHECK-NEXT:    ext v17.16b, v6.16b, v6.16b, #8
; CHECK-NEXT:    str d18, [x9]
; CHECK-NEXT:    addpl x9, sp, #28
; CHECK-NEXT:    ext v18.16b, v7.16b, v7.16b, #8
; CHECK-NEXT:    str d19, [x9]
; CHECK-NEXT:    addpl x9, sp, #31
; CHECK-NEXT:    addpl x9, x9, #5
; CHECK-NEXT:    str d20, [x9]
; CHECK-NEXT:    addpl x9, sp, #31
; CHECK-NEXT:    addpl x9, x9, #13
; CHECK-NEXT:    str d16, [x9]
; CHECK-NEXT:    addpl x9, sp, #31
; CHECK-NEXT:    addpl x9, x9, #21
; CHECK-NEXT:    str d17, [x9]
; CHECK-NEXT:    addpl x9, sp, #31
; CHECK-NEXT:    addpl x9, x9, #29
; CHECK-NEXT:    str d18, [x9]
; CHECK-NEXT:    addvl x9, sp, #1
; CHECK-NEXT:    str d0, [sp]
; CHECK-NEXT:    str d1, [x9]
; CHECK-NEXT:    addvl x9, sp, #2
; CHECK-NEXT:    str d2, [x9]
; CHECK-NEXT:    addvl x9, sp, #3
; CHECK-NEXT:    str d3, [x9]
; CHECK-NEXT:    addvl x9, sp, #4
; CHECK-NEXT:    str d4, [x9]
; CHECK-NEXT:    addvl x9, sp, #5
; CHECK-NEXT:    str d5, [x9]
; CHECK-NEXT:    addvl x9, sp, #6
; CHECK-NEXT:    str d6, [x9]
; CHECK-NEXT:    addvl x9, sp, #7
; CHECK-NEXT:    str d7, [x9]
; CHECK-NEXT:    addpl x9, sp, #28
; CHECK-NEXT:    ld1w { z0.d }, p0/z, [x9, #7, mul vl]
; CHECK-NEXT:    addvl x9, sp, #4
; CHECK-NEXT:    ld1w { z1.d }, p0/z, [sp, #1, mul vl]
; CHECK-NEXT:    ld1w { z3.d }, p0/z, [x9, #7, mul vl]
; CHECK-NEXT:    addvl x9, sp, #3
; CHECK-NEXT:    ld1w { z2.d }, p0/z, [sp, #3, mul vl]
; CHECK-NEXT:    ld1w { z5.d }, p0/z, [x9, #7, mul vl]
; CHECK-NEXT:    addpl x9, sp, #20
; CHECK-NEXT:    ld1w { z4.d }, p0/z, [sp, #5, mul vl]
; CHECK-NEXT:    ld1w { z6.d }, p0/z, [x9, #7, mul vl]
; CHECK-NEXT:    addvl x9, sp, #1
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z0.s
; CHECK-NEXT:    ld1w { z16.d }, p0/z, [x9, #7, mul vl]
; CHECK-NEXT:    addvl x9, sp, #2
; CHECK-NEXT:    fcvtzs z3.d, p0/m, z3.s
; CHECK-NEXT:    ld1w { z17.d }, p0/z, [x9, #7, mul vl]
; CHECK-NEXT:    addpl x9, sp, #12
; CHECK-NEXT:    fcvtzs z5.d, p0/m, z5.s
; CHECK-NEXT:    ld1w { z18.d }, p0/z, [x9, #7, mul vl]
; CHECK-NEXT:    fcvtzs z6.d, p0/m, z6.s
; CHECK-NEXT:    addpl x9, sp, #4
; CHECK-NEXT:    ld1w { z22.d }, p0/z, [x9, #7, mul vl]
; CHECK-NEXT:    ld1w { z7.d }, p0/z, [sp, #7, mul vl]
; CHECK-NEXT:    ld1w { z19.d }, p0/z, [sp]
; CHECK-NEXT:    ld1w { z20.d }, p0/z, [sp, #2, mul vl]
; CHECK-NEXT:    ld1w { z21.d }, p0/z, [sp, #4, mul vl]
; CHECK-NEXT:    ld1w { z23.d }, p0/z, [sp, #6, mul vl]
; CHECK-NEXT:    fcvtzs z18.d, p0/m, z18.s
; CHECK-NEXT:    stp q0, q3, [x8, #224]
; CHECK-NEXT:    movprfx z0, z17
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z17.s
; CHECK-NEXT:    movprfx z3, z22
; CHECK-NEXT:    fcvtzs z3.d, p0/m, z22.s
; CHECK-NEXT:    stp q6, q5, [x8, #192]
; CHECK-NEXT:    movprfx z6, z16
; CHECK-NEXT:    fcvtzs z6.d, p0/m, z16.s
; CHECK-NEXT:    movprfx z5, z23
; CHECK-NEXT:    fcvtzs z5.d, p0/m, z23.s
; CHECK-NEXT:    fcvtzs z7.d, p0/m, z7.s
; CHECK-NEXT:    fcvtzs z2.d, p0/m, z2.s
; CHECK-NEXT:    stp q18, q0, [x8, #160]
; CHECK-NEXT:    movprfx z0, z21
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z21.s
; CHECK-NEXT:    stp q3, q6, [x8, #128]
; CHECK-NEXT:    movprfx z3, z4
; CHECK-NEXT:    fcvtzs z3.d, p0/m, z4.s
; CHECK-NEXT:    movprfx z4, z20
; CHECK-NEXT:    fcvtzs z4.d, p0/m, z20.s
; CHECK-NEXT:    stp q5, q7, [x8, #96]
; CHECK-NEXT:    movprfx z5, z19
; CHECK-NEXT:    fcvtzs z5.d, p0/m, z19.s
; CHECK-NEXT:    stp q4, q2, [x8, #32]
; CHECK-NEXT:    stp q0, q3, [x8, #64]
; CHECK-NEXT:    movprfx z0, z1
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z1.s
; CHECK-NEXT:    stp q5, q0, [x8]
; CHECK-NEXT:    addvl sp, sp, #8
; CHECK-NEXT:    ldr x29, [sp], #16 // 8-byte Folded Reload
; CHECK-NEXT:    ret
  %a = call <32 x i64> @llvm.llrint.v32i64.v32f32(<32 x float> %x)
  ret <32 x i64> %a
}
declare <32 x i64> @llvm.llrint.v32i64.v32f32(<32 x float>)

define <1 x i64> @llrint_v1i64_v1f64(<1 x double> %x) {
; CHECK-LABEL: llrint_v1i64_v1f64:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    // kill: def $d0 killed $d0 def $z0
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z0.d
; CHECK-NEXT:    // kill: def $d0 killed $d0 killed $z0
; CHECK-NEXT:    ret
  %a = call <1 x i64> @llvm.llrint.v1i64.v1f64(<1 x double> %x)
  ret <1 x i64> %a
}
declare <1 x i64> @llvm.llrint.v1i64.v1f64(<1 x double>)

define <2 x i64> @llrint_v2i64_v2f64(<2 x double> %x) {
; CHECK-LABEL: llrint_v2i64_v2f64:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    // kill: def $q0 killed $q0 def $z0
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z0.d
; CHECK-NEXT:    // kill: def $q0 killed $q0 killed $z0
; CHECK-NEXT:    ret
  %a = call <2 x i64> @llvm.llrint.v2i64.v2f64(<2 x double> %x)
  ret <2 x i64> %a
}
declare <2 x i64> @llvm.llrint.v2i64.v2f64(<2 x double>)

define <4 x i64> @llrint_v4i64_v4f64(<4 x double> %x) {
; CHECK-LABEL: llrint_v4i64_v4f64:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    // kill: def $q1 killed $q1 def $z1
; CHECK-NEXT:    // kill: def $q0 killed $q0 def $z0
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z0.d
; CHECK-NEXT:    fcvtzs z1.d, p0/m, z1.d
; CHECK-NEXT:    // kill: def $q0 killed $q0 killed $z0
; CHECK-NEXT:    // kill: def $q1 killed $q1 killed $z1
; CHECK-NEXT:    ret
  %a = call <4 x i64> @llvm.llrint.v4i64.v4f64(<4 x double> %x)
  ret <4 x i64> %a
}
declare <4 x i64> @llvm.llrint.v4i64.v4f64(<4 x double>)

define <8 x i64> @llrint_v8i64_v8f64(<8 x double> %x) {
; CHECK-LABEL: llrint_v8i64_v8f64:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    // kill: def $q3 killed $q3 def $z3
; CHECK-NEXT:    // kill: def $q2 killed $q2 def $z2
; CHECK-NEXT:    // kill: def $q1 killed $q1 def $z1
; CHECK-NEXT:    // kill: def $q0 killed $q0 def $z0
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z0.d
; CHECK-NEXT:    fcvtzs z1.d, p0/m, z1.d
; CHECK-NEXT:    fcvtzs z2.d, p0/m, z2.d
; CHECK-NEXT:    fcvtzs z3.d, p0/m, z3.d
; CHECK-NEXT:    // kill: def $q0 killed $q0 killed $z0
; CHECK-NEXT:    // kill: def $q1 killed $q1 killed $z1
; CHECK-NEXT:    // kill: def $q2 killed $q2 killed $z2
; CHECK-NEXT:    // kill: def $q3 killed $q3 killed $z3
; CHECK-NEXT:    ret
  %a = call <8 x i64> @llvm.llrint.v8i64.v8f64(<8 x double> %x)
  ret <8 x i64> %a
}
declare <8 x i64> @llvm.llrint.v8i64.v8f64(<8 x double>)

define <16 x i64> @llrint_v16f64(<16 x double> %x) {
; CHECK-LABEL: llrint_v16f64:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    // kill: def $q7 killed $q7 def $z7
; CHECK-NEXT:    // kill: def $q6 killed $q6 def $z6
; CHECK-NEXT:    // kill: def $q5 killed $q5 def $z5
; CHECK-NEXT:    // kill: def $q4 killed $q4 def $z4
; CHECK-NEXT:    // kill: def $q3 killed $q3 def $z3
; CHECK-NEXT:    // kill: def $q2 killed $q2 def $z2
; CHECK-NEXT:    // kill: def $q1 killed $q1 def $z1
; CHECK-NEXT:    // kill: def $q0 killed $q0 def $z0
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z0.d
; CHECK-NEXT:    fcvtzs z1.d, p0/m, z1.d
; CHECK-NEXT:    fcvtzs z2.d, p0/m, z2.d
; CHECK-NEXT:    fcvtzs z3.d, p0/m, z3.d
; CHECK-NEXT:    fcvtzs z4.d, p0/m, z4.d
; CHECK-NEXT:    fcvtzs z5.d, p0/m, z5.d
; CHECK-NEXT:    fcvtzs z6.d, p0/m, z6.d
; CHECK-NEXT:    fcvtzs z7.d, p0/m, z7.d
; CHECK-NEXT:    // kill: def $q0 killed $q0 killed $z0
; CHECK-NEXT:    // kill: def $q1 killed $q1 killed $z1
; CHECK-NEXT:    // kill: def $q2 killed $q2 killed $z2
; CHECK-NEXT:    // kill: def $q3 killed $q3 killed $z3
; CHECK-NEXT:    // kill: def $q4 killed $q4 killed $z4
; CHECK-NEXT:    // kill: def $q5 killed $q5 killed $z5
; CHECK-NEXT:    // kill: def $q6 killed $q6 killed $z6
; CHECK-NEXT:    // kill: def $q7 killed $q7 killed $z7
; CHECK-NEXT:    ret
  %a = call <16 x i64> @llvm.llrint.v16i64.v16f64(<16 x double> %x)
  ret <16 x i64> %a
}
declare <16 x i64> @llvm.llrint.v16i64.v16f64(<16 x double>)

define <32 x i64> @llrint_v32f64(<32 x double> %x) {
; CHECK-LABEL: llrint_v32f64:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    ldp q17, q16, [sp, #96]
; CHECK-NEXT:    ldp q19, q18, [sp, #64]
; CHECK-NEXT:    // kill: def $q7 killed $q7 def $z7
; CHECK-NEXT:    // kill: def $q6 killed $q6 def $z6
; CHECK-NEXT:    // kill: def $q5 killed $q5 def $z5
; CHECK-NEXT:    // kill: def $q4 killed $q4 def $z4
; CHECK-NEXT:    // kill: def $q3 killed $q3 def $z3
; CHECK-NEXT:    // kill: def $q2 killed $q2 def $z2
; CHECK-NEXT:    // kill: def $q1 killed $q1 def $z1
; CHECK-NEXT:    // kill: def $q0 killed $q0 def $z0
; CHECK-NEXT:    ldp q21, q20, [sp, #32]
; CHECK-NEXT:    fcvtzs z16.d, p0/m, z16.d
; CHECK-NEXT:    fcvtzs z17.d, p0/m, z17.d
; CHECK-NEXT:    fcvtzs z18.d, p0/m, z18.d
; CHECK-NEXT:    fcvtzs z19.d, p0/m, z19.d
; CHECK-NEXT:    fcvtzs z20.d, p0/m, z20.d
; CHECK-NEXT:    fcvtzs z21.d, p0/m, z21.d
; CHECK-NEXT:    fcvtzs z7.d, p0/m, z7.d
; CHECK-NEXT:    fcvtzs z6.d, p0/m, z6.d
; CHECK-NEXT:    fcvtzs z5.d, p0/m, z5.d
; CHECK-NEXT:    fcvtzs z4.d, p0/m, z4.d
; CHECK-NEXT:    fcvtzs z3.d, p0/m, z3.d
; CHECK-NEXT:    fcvtzs z2.d, p0/m, z2.d
; CHECK-NEXT:    str q16, [x8, #240]
; CHECK-NEXT:    ldp q22, q16, [sp]
; CHECK-NEXT:    stp q18, q17, [x8, #208]
; CHECK-NEXT:    fcvtzs z1.d, p0/m, z1.d
; CHECK-NEXT:    fcvtzs z0.d, p0/m, z0.d
; CHECK-NEXT:    stp q5, q6, [x8, #80]
; CHECK-NEXT:    fcvtzs z16.d, p0/m, z16.d
; CHECK-NEXT:    movprfx z17, z22
; CHECK-NEXT:    fcvtzs z17.d, p0/m, z22.d
; CHECK-NEXT:    stp q3, q4, [x8, #48]
; CHECK-NEXT:    stp q20, q19, [x8, #176]
; CHECK-NEXT:    stp q1, q2, [x8, #16]
; CHECK-NEXT:    str q0, [x8]
; CHECK-NEXT:    stp q7, q17, [x8, #112]
; CHECK-NEXT:    stp q16, q21, [x8, #144]
; CHECK-NEXT:    ret
  %a = call <32 x i64> @llvm.llrint.v32i64.v16f64(<32 x double> %x)
  ret <32 x i64> %a
}
declare <32 x i64> @llvm.llrint.v32i64.v32f64(<32 x double>)
