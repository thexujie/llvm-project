; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 4
; REQUIRES: asserts
; RUN: opt -S -passes=loop-vectorize < %s 2>&1 | FileCheck %s

target datalayout = "e-m:e-p:64:64-i64:64-i128:128-n32:64-S128"

define i32 @truncate_umax() #1 {
; CHECK-LABEL: define i32 @truncate_umax(
; CHECK-SAME: ) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i1> @llvm.umax.v4i1(<4 x i1> zeroinitializer, <4 x i1> zeroinitializer)
; CHECK-NEXT:    [[TMP1:%.*]] = icmp ne <4 x i1> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP2:%.*]] = zext <4 x i1> [[TMP1]] to <4 x i32>
; CHECK-NEXT:    [[TMP3:%.*]] = shl <4 x i32> [[TMP2]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP4:%.*]] = trunc <4 x i32> [[TMP3]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP5:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP5]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i8> [[TMP4]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.umax.i64(i64 [[ZEXT_0]], i64 0)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP3:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP6]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.umax.i64(i64 %zext.0, i64 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_umin() #1 {
; CHECK-LABEL: define i32 @truncate_umin(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i1> @llvm.umin.v4i1(<4 x i1> zeroinitializer, <4 x i1> zeroinitializer)
; CHECK-NEXT:    [[TMP1:%.*]] = icmp ne <4 x i1> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP2:%.*]] = zext <4 x i1> [[TMP1]] to <4 x i32>
; CHECK-NEXT:    [[TMP3:%.*]] = shl <4 x i32> [[TMP2]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP4:%.*]] = trunc <4 x i32> [[TMP3]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP5:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP5]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i8> [[TMP4]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.umin.i64(i64 [[ZEXT_0]], i64 0)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP5:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP6]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.umin.i64(i64 %zext.0, i64 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_smax() #1 {
; CHECK-LABEL: define i32 @truncate_smax(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i1> @llvm.smax.v4i1(<4 x i1> zeroinitializer, <4 x i1> zeroinitializer)
; CHECK-NEXT:    [[TMP1:%.*]] = icmp ne <4 x i1> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP2:%.*]] = zext <4 x i1> [[TMP1]] to <4 x i32>
; CHECK-NEXT:    [[TMP3:%.*]] = shl <4 x i32> [[TMP2]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP4:%.*]] = trunc <4 x i32> [[TMP3]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP5:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP5]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP6:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i8> [[TMP4]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.smax.i64(i64 [[ZEXT_0]], i64 0)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP7:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP6]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.smax.i64(i64 %zext.0, i64 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_smin() #1 {
; CHECK-LABEL: define i32 @truncate_smin(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i1> @llvm.smin.v4i1(<4 x i1> zeroinitializer, <4 x i1> zeroinitializer)
; CHECK-NEXT:    [[TMP1:%.*]] = icmp ne <4 x i1> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP2:%.*]] = zext <4 x i1> [[TMP1]] to <4 x i32>
; CHECK-NEXT:    [[TMP3:%.*]] = shl <4 x i32> [[TMP2]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP4:%.*]] = trunc <4 x i32> [[TMP3]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP5:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP5]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP8:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i8> [[TMP4]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.smin.i64(i64 [[ZEXT_0]], i64 0)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP9:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP6]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.smin.i64(i64 %zext.0, i64 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_fshl() #1 {
; CHECK-LABEL: define i32 @truncate_fshl(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i1> @llvm.fshl.v4i1(<4 x i1> zeroinitializer, <4 x i1> zeroinitializer, <4 x i1> zeroinitializer)
; CHECK-NEXT:    [[TMP1:%.*]] = icmp ne <4 x i1> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP2:%.*]] = zext <4 x i1> [[TMP1]] to <4 x i32>
; CHECK-NEXT:    [[TMP3:%.*]] = shl <4 x i32> [[TMP2]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP4:%.*]] = trunc <4 x i32> [[TMP3]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP5:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP5]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP10:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i8> [[TMP4]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.fshl.i64(i64 [[ZEXT_0]], i64 0, i64 0)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP11:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP6]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.fshl.i64(i64 %zext.0, i64 0, i64 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_fshr() #1 {
; CHECK-LABEL: define i32 @truncate_fshr(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i1> @llvm.fshr.v4i1(<4 x i1> zeroinitializer, <4 x i1> zeroinitializer, <4 x i1> zeroinitializer)
; CHECK-NEXT:    [[TMP1:%.*]] = icmp ne <4 x i1> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP2:%.*]] = zext <4 x i1> [[TMP1]] to <4 x i32>
; CHECK-NEXT:    [[TMP3:%.*]] = shl <4 x i32> [[TMP2]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP4:%.*]] = trunc <4 x i32> [[TMP3]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP5:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP5]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP12:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i8> [[TMP4]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.fshr.i64(i64 [[ZEXT_0]], i64 0, i64 0)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP13:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP6]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.fshr.i64(i64 %zext.0, i64 0, i64 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_cttz() #1 {
; CHECK-LABEL: define i32 @truncate_cttz(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i1> @llvm.cttz.v4i1(<4 x i1> zeroinitializer, i1 false)
; CHECK-NEXT:    [[TMP1:%.*]] = icmp ne <4 x i1> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP2:%.*]] = zext <4 x i1> [[TMP1]] to <4 x i32>
; CHECK-NEXT:    [[TMP3:%.*]] = shl <4 x i32> [[TMP2]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP4:%.*]] = trunc <4 x i32> [[TMP3]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP5:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP5]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP14:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i8> [[TMP4]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.cttz.i64(i64 [[ZEXT_0]], i1 false)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP15:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP6]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.cttz.i64(i64 %zext.0, i1 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_ctlz() #1 {
; CHECK-LABEL: define i32 @truncate_ctlz(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i1> @llvm.ctlz.v4i1(<4 x i1> zeroinitializer, i1 false)
; CHECK-NEXT:    [[TMP1:%.*]] = icmp ne <4 x i1> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP2:%.*]] = zext <4 x i1> [[TMP1]] to <4 x i32>
; CHECK-NEXT:    [[TMP3:%.*]] = shl <4 x i32> [[TMP2]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP4:%.*]] = trunc <4 x i32> [[TMP3]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP5:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP5]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP16:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i8> [[TMP4]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.ctlz.i64(i64 [[ZEXT_0]], i1 false)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP17:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP6]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.ctlz.i64(i64 %zext.0, i1 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_bitreverse() #1 {
; CHECK-LABEL: define i32 @truncate_bitreverse(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i1> @llvm.bitreverse.v4i1(<4 x i1> zeroinitializer)
; CHECK-NEXT:    [[TMP1:%.*]] = icmp ne <4 x i1> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP2:%.*]] = zext <4 x i1> [[TMP1]] to <4 x i32>
; CHECK-NEXT:    [[TMP3:%.*]] = shl <4 x i32> [[TMP2]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP4:%.*]] = trunc <4 x i32> [[TMP3]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP5:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP5]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP18:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i8> [[TMP4]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.bitreverse.i64(i64 [[ZEXT_0]])
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP19:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP6]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.bitreverse.i64(i64 %zext.0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_bswap() #1 {
; CHECK-LABEL: define i32 @truncate_bswap(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i16> @llvm.bswap.v4i16(<4 x i16> zeroinitializer)
; CHECK-NEXT:    [[TMP1:%.*]] = icmp ne <4 x i16> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP2:%.*]] = zext <4 x i1> [[TMP1]] to <4 x i32>
; CHECK-NEXT:    [[TMP3:%.*]] = shl <4 x i32> [[TMP2]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP4:%.*]] = trunc <4 x i32> [[TMP3]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP5:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP5]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP20:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i8> [[TMP4]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.bswap.i64(i64 [[ZEXT_0]])
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP21:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP6]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.bswap.i64(i64 %zext.0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

; Unsupported intrinsics

define i32 @truncate_sadd_sat() #1 {
; CHECK-LABEL: define i32 @truncate_sadd_sat(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i64> @llvm.sadd.sat.v4i64(<4 x i64> zeroinitializer, <4 x i64> zeroinitializer)
; CHECK-NEXT:    [[TMP1:%.*]] = trunc <4 x i64> [[TMP0]] to <4 x i1>
; CHECK-NEXT:    [[TMP2:%.*]] = icmp ne <4 x i1> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP3:%.*]] = zext <4 x i1> [[TMP2]] to <4 x i32>
; CHECK-NEXT:    [[TMP4:%.*]] = shl <4 x i32> [[TMP3]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP5:%.*]] = trunc <4 x i32> [[TMP4]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP6:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP6]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP22:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i8> [[TMP5]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.sadd.sat.i64(i64 [[ZEXT_0]], i64 0)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP23:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP7]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.sadd.sat.i64(i64 %zext.0, i64 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_uadd_sat() #1 {
; CHECK-LABEL: define i32 @truncate_uadd_sat(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i64> @llvm.uadd.sat.v4i64(<4 x i64> zeroinitializer, <4 x i64> zeroinitializer)
; CHECK-NEXT:    [[TMP1:%.*]] = trunc <4 x i64> [[TMP0]] to <4 x i1>
; CHECK-NEXT:    [[TMP2:%.*]] = icmp ne <4 x i1> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP3:%.*]] = zext <4 x i1> [[TMP2]] to <4 x i32>
; CHECK-NEXT:    [[TMP4:%.*]] = shl <4 x i32> [[TMP3]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP5:%.*]] = trunc <4 x i32> [[TMP4]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP6:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP6]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP24:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i8> [[TMP5]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.uadd.sat.i64(i64 [[ZEXT_0]], i64 0)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP25:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP7]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.uadd.sat.i64(i64 %zext.0, i64 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_ssub_sat() #1 {
; CHECK-LABEL: define i32 @truncate_ssub_sat(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i64> @llvm.uadd.sat.v4i64(<4 x i64> zeroinitializer, <4 x i64> zeroinitializer)
; CHECK-NEXT:    [[TMP1:%.*]] = trunc <4 x i64> [[TMP0]] to <4 x i1>
; CHECK-NEXT:    [[TMP2:%.*]] = icmp ne <4 x i1> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP3:%.*]] = zext <4 x i1> [[TMP2]] to <4 x i32>
; CHECK-NEXT:    [[TMP4:%.*]] = shl <4 x i32> [[TMP3]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP5:%.*]] = trunc <4 x i32> [[TMP4]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP6:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP6]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP26:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i8> [[TMP5]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.uadd.sat.i64(i64 [[ZEXT_0]], i64 0)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP27:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP7]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.uadd.sat.i64(i64 %zext.0, i64 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_usub_sat() #1 {
; CHECK-LABEL: define i32 @truncate_usub_sat(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call <4 x i64> @llvm.usub.sat.v4i64(<4 x i64> zeroinitializer, <4 x i64> zeroinitializer)
; CHECK-NEXT:    [[TMP1:%.*]] = trunc <4 x i64> [[TMP0]] to <4 x i1>
; CHECK-NEXT:    [[TMP2:%.*]] = icmp ne <4 x i1> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP3:%.*]] = zext <4 x i1> [[TMP2]] to <4 x i32>
; CHECK-NEXT:    [[TMP4:%.*]] = shl <4 x i32> [[TMP3]], <i32 8, i32 8, i32 8, i32 8>
; CHECK-NEXT:    [[TMP5:%.*]] = trunc <4 x i32> [[TMP4]] to <4 x i8>
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP6:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP6]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP28:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i8> [[TMP5]], i32 3
; CHECK-NEXT:    br i1 false, label [[LOOP_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.usub.sat.i64(i64 [[ZEXT_0]], i64 0)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT]], !llvm.loop [[LOOP29:![0-9]+]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ], [ [[TMP7]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.usub.sat.i64(i64 %zext.0, i64 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_sshl_sat() #1 {
; CHECK-LABEL: define i32 @truncate_sshl_sat(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.sshl.sat.i64(i64 [[ZEXT_0]], i64 0)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT:%.*]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.sshl.sat.i64(i64 %zext.0, i64 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

define i32 @truncate_ushl_sat() #1 {
; CHECK-LABEL: define i32 @truncate_ushl_sat(
; CHECK-SAME: ) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[PHI_0:%.*]] = phi i64 [ [[INCREMENTOR:%.*]], [[LOOP]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    [[INCREMENTOR]] = add i64 [[PHI_0]], 1
; CHECK-NEXT:    [[ZEXT_0:%.*]] = zext i8 0 to i64
; CHECK-NEXT:    [[INTRINSIC_0:%.*]] = tail call i64 @llvm.ushl.sat.i64(i64 [[ZEXT_0]], i64 0)
; CHECK-NEXT:    [[CMP_0:%.*]] = icmp ne i64 [[INTRINSIC_0]], 0
; CHECK-NEXT:    [[ZEXT_1:%.*]] = zext i1 [[CMP_0]] to i64
; CHECK-NEXT:    [[TRUNC_0:%.*]] = trunc i64 [[ZEXT_1]] to i32
; CHECK-NEXT:    [[SHL_0:%.*]] = shl i32 [[TRUNC_0]], 8
; CHECK-NEXT:    [[TRUNC_1:%.*]] = trunc i32 [[SHL_0]] to i8
; CHECK-NEXT:    [[EXITCOND6:%.*]] = icmp ne i64 [[PHI_0]], 16
; CHECK-NEXT:    br i1 [[EXITCOND6]], label [[LOOP]], label [[LOOP_EXIT:%.*]]
; CHECK:       loop.exit:
; CHECK-NEXT:    [[TRUNC_1_LCSSA:%.*]] = phi i8 [ [[TRUNC_1]], [[LOOP]] ]
; CHECK-NEXT:    store i8 [[TRUNC_1_LCSSA]], ptr null, align 1
; CHECK-NEXT:    ret i32 0
;
entry:
  br label %loop

loop:                             ; preds = %loop, %entry
  %phi.0 = phi i64 [ %incrementor, %loop ], [ 0, %entry ]
  %incrementor = add i64 %phi.0, 1

  %zext.0 = zext i8 0 to i64
  %intrinsic.0 = tail call i64 @llvm.ushl.sat.i64(i64 %zext.0, i64 0)
  %cmp.0 = icmp ne i64 %intrinsic.0, 0
  %zext.1 = zext i1 %cmp.0 to i64
  %trunc.0 = trunc i64 %zext.1 to i32
  %shl.0 = shl i32 %trunc.0, 8 ; Shift and truncate to remove any alive bits
  %trunc.1 = trunc i32 %shl.0 to i8

  %exitcond6 = icmp ne i64 %phi.0, 16
  br i1 %exitcond6, label %loop, label %loop.exit

loop.exit:                           ; preds = %loop
  store i8 %trunc.1, ptr null, align 1
  ret i32 0
}

attributes #1 = { "target-features"="+v" }
;.
; CHECK: [[LOOP0]] = distinct !{[[LOOP0]], [[META1:![0-9]+]], [[META2:![0-9]+]]}
; CHECK: [[META1]] = !{!"llvm.loop.isvectorized", i32 1}
; CHECK: [[META2]] = !{!"llvm.loop.unroll.runtime.disable"}
; CHECK: [[LOOP3]] = distinct !{[[LOOP3]], [[META2]], [[META1]]}
; CHECK: [[LOOP4]] = distinct !{[[LOOP4]], [[META1]], [[META2]]}
; CHECK: [[LOOP5]] = distinct !{[[LOOP5]], [[META2]], [[META1]]}
; CHECK: [[LOOP6]] = distinct !{[[LOOP6]], [[META1]], [[META2]]}
; CHECK: [[LOOP7]] = distinct !{[[LOOP7]], [[META2]], [[META1]]}
; CHECK: [[LOOP8]] = distinct !{[[LOOP8]], [[META1]], [[META2]]}
; CHECK: [[LOOP9]] = distinct !{[[LOOP9]], [[META2]], [[META1]]}
; CHECK: [[LOOP10]] = distinct !{[[LOOP10]], [[META1]], [[META2]]}
; CHECK: [[LOOP11]] = distinct !{[[LOOP11]], [[META2]], [[META1]]}
; CHECK: [[LOOP12]] = distinct !{[[LOOP12]], [[META1]], [[META2]]}
; CHECK: [[LOOP13]] = distinct !{[[LOOP13]], [[META2]], [[META1]]}
; CHECK: [[LOOP14]] = distinct !{[[LOOP14]], [[META1]], [[META2]]}
; CHECK: [[LOOP15]] = distinct !{[[LOOP15]], [[META2]], [[META1]]}
; CHECK: [[LOOP16]] = distinct !{[[LOOP16]], [[META1]], [[META2]]}
; CHECK: [[LOOP17]] = distinct !{[[LOOP17]], [[META2]], [[META1]]}
; CHECK: [[LOOP18]] = distinct !{[[LOOP18]], [[META1]], [[META2]]}
; CHECK: [[LOOP19]] = distinct !{[[LOOP19]], [[META2]], [[META1]]}
; CHECK: [[LOOP20]] = distinct !{[[LOOP20]], [[META1]], [[META2]]}
; CHECK: [[LOOP21]] = distinct !{[[LOOP21]], [[META2]], [[META1]]}
; CHECK: [[LOOP22]] = distinct !{[[LOOP22]], [[META1]], [[META2]]}
; CHECK: [[LOOP23]] = distinct !{[[LOOP23]], [[META2]], [[META1]]}
; CHECK: [[LOOP24]] = distinct !{[[LOOP24]], [[META1]], [[META2]]}
; CHECK: [[LOOP25]] = distinct !{[[LOOP25]], [[META2]], [[META1]]}
; CHECK: [[LOOP26]] = distinct !{[[LOOP26]], [[META1]], [[META2]]}
; CHECK: [[LOOP27]] = distinct !{[[LOOP27]], [[META2]], [[META1]]}
; CHECK: [[LOOP28]] = distinct !{[[LOOP28]], [[META1]], [[META2]]}
; CHECK: [[LOOP29]] = distinct !{[[LOOP29]], [[META2]], [[META1]]}
;.
